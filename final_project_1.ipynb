{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final project implement a transformer from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### steps of the final project :\n",
    "- Import necessary libraries and modules\n",
    "- Define the basic building blocks: Multi-Head Attention, Position-wise Feed-Forward Networks, Positional Encoding\n",
    "- Build the Encoder and Decoder layers\n",
    "- Combine Encoder and Decoder layers to create the complete Transformer model\n",
    "- Prepare your data [ clean and toknize and do all the cleaning steps nessacry from your point of veiw]\n",
    "- Train the model\n",
    "- test the model \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libararies Needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for data reading and slicing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# for string manipulate\n",
    "import string\n",
    "import re\n",
    "\n",
    "# for handling contractions\n",
    "import contractions\n",
    "\n",
    "# Import Natural language Toolkit for stopwords and tokization and stemming and lemmatization\n",
    "import nltk\n",
    "\n",
    "# Import Natural language Toolkit for stopwords \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import Natural language Toolkit for tokization \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Import Natural language Toolkit for lemmatization \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# for train and test split \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for randomly selecting the data\n",
    "import random\n",
    "\n",
    "# for transformer Neural Networks inside it using pytorch\n",
    "import torch\n",
    "\n",
    "# for pytorch neural network\n",
    "import torch.nn as nn\n",
    "\n",
    "# for pytorch optimizer\n",
    "import torch.optim as optim\n",
    "\n",
    "# for pytorch convert data to tensor and create train and test data\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# for pytorch activation ,loss functions\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# for evaluating the model \n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK tokenizer data\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK Stop Words data\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the WordNet dataset\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Analyzing and Understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read CSV \n",
    "df = pd.read_csv(r\"C:\\Users\\User\\Desktop\\koleya\\summer 2\\SPRINTS  AI & ML program\\Assigment\\assigment 6\\train.csv\")\n",
    "\n",
    "# look inside the first 10 rows of data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get shape of data\n",
    "df.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is 159571 rows and 8 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   id             159571 non-null  object\n",
      " 1   comment_text   159571 non-null  object\n",
      " 2   toxic          159571 non-null  int64 \n",
      " 3   severe_toxic   159571 non-null  int64 \n",
      " 4   obscene        159571 non-null  int64 \n",
      " 5   threat         159571 non-null  int64 \n",
      " 6   insult         159571 non-null  int64 \n",
      " 7   identity_hate  159571 non-null  int64 \n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 9.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# get info of data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is no nulls in data and all columns are int except columns : id , comment_text are string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checking for nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               0\n",
       "comment_text     0\n",
       "toxic            0\n",
       "severe_toxic     0\n",
       "obscene          0\n",
       "threat           0\n",
       "insult           0\n",
       "identity_hate    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the number of nulls in each column\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### there is no nulls in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describtion of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>toxic</th>\n",
       "      <td>159571.0</td>\n",
       "      <td>0.095844</td>\n",
       "      <td>0.294379</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>severe_toxic</th>\n",
       "      <td>159571.0</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obscene</th>\n",
       "      <td>159571.0</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>0.223931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>threat</th>\n",
       "      <td>159571.0</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.054650</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>insult</th>\n",
       "      <td>159571.0</td>\n",
       "      <td>0.049364</td>\n",
       "      <td>0.216627</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>identity_hate</th>\n",
       "      <td>159571.0</td>\n",
       "      <td>0.008805</td>\n",
       "      <td>0.093420</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  count      mean       std  min  25%  50%  75%  max\n",
       "toxic          159571.0  0.095844  0.294379  0.0  0.0  0.0  0.0  1.0\n",
       "severe_toxic   159571.0  0.009996  0.099477  0.0  0.0  0.0  0.0  1.0\n",
       "obscene        159571.0  0.052948  0.223931  0.0  0.0  0.0  0.0  1.0\n",
       "threat         159571.0  0.002996  0.054650  0.0  0.0  0.0  0.0  1.0\n",
       "insult         159571.0  0.049364  0.216627  0.0  0.0  0.0  0.0  1.0\n",
       "identity_hate  159571.0  0.008805  0.093420  0.0  0.0  0.0  0.0  1.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the describtion of statistics of the int columns\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All int columns are binary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id               object\n",
      "comment_text     object\n",
      "toxic             int64\n",
      "severe_toxic      int64\n",
      "obscene           int64\n",
      "threat            int64\n",
      "insult            int64\n",
      "identity_hate     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# get types of each column\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All columns are int except ID and comment_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Duplicates in column id and comment_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop identical rows in column comment_text\n",
    "df.drop_duplicates(subset='comment_text',inplace=True) \n",
    "\n",
    "# drop identical rows in column id\n",
    "df.drop_duplicates(subset='id',inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 8)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There is no duplicantes in columns : comment_text , id ; because number of rows is the same before drop duplicants and  after drop duplicants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See unique values in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id               159571\n",
       "comment_text     159571\n",
       "toxic                 2\n",
       "severe_toxic          2\n",
       "obscene               2\n",
       "threat                2\n",
       "insult                2\n",
       "identity_hate         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the number of unique value of each column\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### All rows in columns : id , comment_text has a unique value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See what is the unique values in each int column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic\n",
       "0    144277\n",
       "1     15294\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see each unique value in toxic quantity of it\n",
    "df['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "severe_toxic\n",
       "0    157976\n",
       "1      1595\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see each unique value in severe_toxic quantity of it\n",
    "df['severe_toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "obscene\n",
       "0    151122\n",
       "1      8449\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see each unique value in obscene quantity of it\n",
    "df['obscene'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "threat\n",
       "0    159093\n",
       "1       478\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see each unique value in threat quantity of it\n",
    "df['threat'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "insult\n",
       "0    151694\n",
       "1      7877\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see each unique value in insult quantity of it\n",
    "df['insult'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "identity_hate\n",
       "0    158166\n",
       "1      1405\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see each unique value in identity_hate quantity of it\n",
    "df['identity_hate'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I can say from in the above cells that all rows has unique id and comment_text , and the rest columns have only zero or one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See if is there rows that has inappropriate comment and how many of the data it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>143346 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "0       0000997932d777bf  Explanation\\nWhy the edits made under my usern...   \n",
       "1       000103f0d9cfb60f  D'aww! He matches this background colour I'm s...   \n",
       "2       000113f07ec002fd  Hey man, I'm really not trying to edit war. It...   \n",
       "3       0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4       0001d958c54c6e35  You, sir, are my hero. Any chance you remember...   \n",
       "...                  ...                                                ...   \n",
       "159566  ffe987279560d7ff  \":::::And for the second time of asking, when ...   \n",
       "159567  ffea4adeee384e90  You should be ashamed of yourself \\n\\nThat is ...   \n",
       "159568  ffee36eab5c267c9  Spitzer \\n\\nUmm, theres no actual article for ...   \n",
       "159569  fff125370e4aaaf3  And it looks like it was actually you who put ...   \n",
       "159570  fff46fc426af1f9a  \"\\nAnd ... I really don't think you understand...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0           0             0        0       0       0              0  \n",
       "1           0             0        0       0       0              0  \n",
       "2           0             0        0       0       0              0  \n",
       "3           0             0        0       0       0              0  \n",
       "4           0             0        0       0       0              0  \n",
       "...       ...           ...      ...     ...     ...            ...  \n",
       "159566      0             0        0       0       0              0  \n",
       "159567      0             0        0       0       0              0  \n",
       "159568      0             0        0       0       0              0  \n",
       "159569      0             0        0       0       0              0  \n",
       "159570      0             0        0       0       0              0  \n",
       "\n",
       "[143346 rows x 8 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the rows that is appropriate by making a condition that all int values = 0\n",
    "condition = (df[\"toxic\"]==0)&(df[\"severe_toxic\"]==0)&(df[\"obscene\"]==0)&(df[\"threat\"]==0)&(df[\"insult\"]==0)&(df[\"identity_hate\"]==0)\n",
    "\n",
    "df[condition]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of clean data 89.83211235124176 % \n",
      "Percentage of toxic data 10.167887648758239 %\n"
     ]
    }
   ],
   "source": [
    "# get the number of rows that meets the condition\n",
    "num = df[condition].shape[0] \n",
    "\n",
    "# divide the number by whole length of data * 100 to get percentage\n",
    "Percentage_of_clean_data = (num / len(df))*100 \n",
    "\n",
    "# get the percentage of appropriate data\n",
    "Percentage_of_toxic_data = 100 - Percentage_of_clean_data\n",
    "\n",
    "print(\"Percentage of clean data {} % \\nPercentage of toxic data {} %\".format(Percentage_of_clean_data,Percentage_of_toxic_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About 90% of the data containes appropriate comments and 10 % is inappropriate comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### steps of cleaning the data :\n",
    "- lowercase the data : to ensure consistency\n",
    "- Removing URLs\n",
    "- Handling Contractions , for example change \"can't\" to \"cannot\"\n",
    "- Removing Special Characters \n",
    "- Remove punctuation\n",
    "- Remove Numbers\n",
    "- stop words : to reduce the input's dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of a function that counts the rows that have  ( URLs , Special Characters , Punctuation , Stop Words , Numbers )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_rows(column):\n",
    "    # Initialize count = zero\n",
    "    urls_count = 0\n",
    "    special_chars_count = 0\n",
    "    punctuation_count = 0\n",
    "    stop_word_count = 0\n",
    "    numeric_count = 0\n",
    "    \n",
    "    # make a set of all stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    for text in column:\n",
    "        # Count URLs\n",
    "        # if URLs found\n",
    "        if re.search(r'http\\S+|www\\S+|https\\S+', text):\n",
    "        # add by one\n",
    "            urls_count += 1\n",
    "\n",
    "        # Count special characters except english letters and spaces\n",
    "        # if special characters found \n",
    "        if re.search(r'[^A-Za-z0-9\\s]+', text):\n",
    "        # add by one\n",
    "            special_chars_count += 1\n",
    "        \n",
    "        # Count numbers\n",
    "        # if number found \n",
    "        if  re.search(r'\\d', text):\n",
    "        # add by one\n",
    "            numeric_count +=1\n",
    "\n",
    "        # Count punctuation\n",
    "        # if punctuation found \n",
    "        if any(char in string.punctuation for char in text):\n",
    "        # add by one\n",
    "            punctuation_count += 1\n",
    "\n",
    "        # Count stop words\n",
    "        words = text.split()\n",
    "        # if the word is stop word found \n",
    "        if any(word.lower() in stop_words for word in words):\n",
    "        # add by one\n",
    "            stop_word_count += 1\n",
    "\n",
    "    return urls_count, special_chars_count , punctuation_count , stop_word_count , numeric_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of a function that lowercase the data , Remove URLs , Return the word to it's basic form  , Remove Special Characters , Remove punctuation , Remove Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(text):\n",
    "\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Handle Contractions\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove any special characters except english letters and spaces \n",
    "    text=re.sub(r'[^A-Za-z\\s]+',' ',text)\n",
    "    \n",
    "    # Remove stop words\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # create a set of stop words using NLTK libarary\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    # iterate over the data and remove any stop word in it\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Join the filtered tokens back into a string\n",
    "    text = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count the number of rows that have URLs , Special Characters , punctuation , numbers , and stop words BEFORE clean data function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with URLs: 5151\n",
      "Number of rows with special characters: 155146\n",
      "Number of rows with punctuation: 155106\n",
      "Number of rows with stop words: 154877\n",
      "Number of rows with Numbers: 51212\n"
     ]
    }
   ],
   "source": [
    "# apply the counting count_rows_urls_and_special_chars_and_punctuation_and_stop_words_and_numbers functions\n",
    "urls_count, special_chars_count , punctuation_count , stop_word_count , numeric_count= count_rows(df['comment_text'])\n",
    "\n",
    "print(f\"Number of rows with URLs: {urls_count}\")\n",
    "print(f\"Number of rows with special characters: {special_chars_count}\")\n",
    "print(f\"Number of rows with punctuation: {punctuation_count}\")\n",
    "print(f\"Number of rows with stop words: {stop_word_count}\")\n",
    "print(f\"Number of rows with Numbers: {numeric_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply clean data function on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply clean_data function on comment_text column\n",
    "df['comment_text'] = df['comment_text'].apply(clean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count the number of rows that have URLs , Special Characters , punctuation , numbers , and stop words AFTER clean data function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with URLs: 0\n",
      "Number of rows with special characters: 0\n",
      "Number of rows with punctuation: 0\n",
      "Number of rows with stop words: 0\n",
      "Number of rows with Numbers: 0\n"
     ]
    }
   ],
   "source": [
    "# apply the counting count_rows_urls_and_special_chars_and_punctuation_and_stop_words_and_numbers functions\n",
    "urls_count, special_chars_count , punctuation_count , stop_word_count , numeric_count= count_rows(df['comment_text'])\n",
    "\n",
    "print(f\"Number of rows with URLs: {urls_count}\")\n",
    "print(f\"Number of rows with special characters: {special_chars_count}\")\n",
    "print(f\"Number of rows with punctuation: {punctuation_count}\")\n",
    "print(f\"Number of rows with stop words: {stop_word_count}\")\n",
    "print(f\"Number of rows with Numbers: {numeric_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The function successfully cleaned the data from URLs , Special Characters , punctuation , numbers , and stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>daww matches background colour seemingly stuck...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>make real suggestions improvement wondered sec...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>second time asking view completely contradicts...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>ashamed horrible thing put talk page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>spitzer umm actual article prostitution ring c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>looks like actually put speedy first version d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>really think understand came idea bad right aw...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "0       0000997932d777bf  explanation edits made username hardcore metal...   \n",
       "1       000103f0d9cfb60f  daww matches background colour seemingly stuck...   \n",
       "2       000113f07ec002fd  hey man really trying edit war guy constantly ...   \n",
       "3       0001b41b1c6bb37e  make real suggestions improvement wondered sec...   \n",
       "4       0001d958c54c6e35                      sir hero chance remember page   \n",
       "...                  ...                                                ...   \n",
       "159566  ffe987279560d7ff  second time asking view completely contradicts...   \n",
       "159567  ffea4adeee384e90               ashamed horrible thing put talk page   \n",
       "159568  ffee36eab5c267c9  spitzer umm actual article prostitution ring c...   \n",
       "159569  fff125370e4aaaf3  looks like actually put speedy first version d...   \n",
       "159570  fff46fc426af1f9a  really think understand came idea bad right aw...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0           0             0        0       0       0              0  \n",
       "1           0             0        0       0       0              0  \n",
       "2           0             0        0       0       0              0  \n",
       "3           0             0        0       0       0              0  \n",
       "4           0             0        0       0       0              0  \n",
       "...       ...           ...      ...     ...     ...            ...  \n",
       "159566      0             0        0       0       0              0  \n",
       "159567      0             0        0       0       0              0  \n",
       "159568      0             0        0       0       0              0  \n",
       "159569      0             0        0       0       0              0  \n",
       "159570      0             0        0       0       0              0  \n",
       "\n",
       "[159571 rows x 8 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Duplicates After Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cleaning function will make some rows hold the same text , so will drop duplicantes on column comment text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply function on text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>daww matches background colour seemingly stuck...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>make real suggestions improvement wondered sec...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157664</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>second time asking view completely contradicts...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157665</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>ashamed horrible thing put talk page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157666</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>spitzer umm actual article prostitution ring c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157667</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>looks like actually put speedy first version d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157668</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>really think understand came idea bad right aw...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157669 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "0       0000997932d777bf  explanation edits made username hardcore metal...   \n",
       "1       000103f0d9cfb60f  daww matches background colour seemingly stuck...   \n",
       "2       000113f07ec002fd  hey man really trying edit war guy constantly ...   \n",
       "3       0001b41b1c6bb37e  make real suggestions improvement wondered sec...   \n",
       "4       0001d958c54c6e35                      sir hero chance remember page   \n",
       "...                  ...                                                ...   \n",
       "157664  ffe987279560d7ff  second time asking view completely contradicts...   \n",
       "157665  ffea4adeee384e90               ashamed horrible thing put talk page   \n",
       "157666  ffee36eab5c267c9  spitzer umm actual article prostitution ring c...   \n",
       "157667  fff125370e4aaaf3  looks like actually put speedy first version d...   \n",
       "157668  fff46fc426af1f9a  really think understand came idea bad right aw...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0           0             0        0       0       0              0  \n",
       "1           0             0        0       0       0              0  \n",
       "2           0             0        0       0       0              0  \n",
       "3           0             0        0       0       0              0  \n",
       "4           0             0        0       0       0              0  \n",
       "...       ...           ...      ...     ...     ...            ...  \n",
       "157664      0             0        0       0       0              0  \n",
       "157665      0             0        0       0       0              0  \n",
       "157666      0             0        0       0       0              0  \n",
       "157667      0             0        0       0       0              0  \n",
       "157668      0             0        0       0       0              0  \n",
       "\n",
       "[157669 rows x 8 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop duplicates on comment text column\n",
    "df.drop_duplicates(subset='comment_text',inplace=True) \n",
    "\n",
    "# reset index and drop column index that will be produced from reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See the number of rows has reduced from 159571 to 157669 , so there was duplicantes in comment_text column after cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toknization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that tokenize the data which is built in \n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply function on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function on mentioned column \n",
    "df['tokenized_column'] = df['comment_text'].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>tokenized_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[explanation, edits, made, username, hardcore,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>daww matches background colour seemingly stuck...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[daww, matches, background, colour, seemingly,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[hey, man, really, trying, edit, war, guy, con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>make real suggestions improvement wondered sec...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[make, real, suggestions, improvement, wondere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[sir, hero, chance, remember, page]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00025465d4725e87</td>\n",
       "      <td>congratulations well use tools well talk</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[congratulations, well, use, tools, well, talk]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>cocksucker piss around work</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[cocksucker, piss, around, work]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00031b1e95af7921</td>\n",
       "      <td>vandalism matt shirvington article reverted pl...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[vandalism, matt, shirvington, article, revert...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00037261f536c51d</td>\n",
       "      <td>sorry word nonsense offensive anyway intending...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[sorry, word, nonsense, offensive, anyway, int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00040093b2687caa</td>\n",
       "      <td>alignment subject contrary dulithgow</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[alignment, subject, contrary, dulithgow]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  explanation edits made username hardcore metal...      0   \n",
       "1  000103f0d9cfb60f  daww matches background colour seemingly stuck...      0   \n",
       "2  000113f07ec002fd  hey man really trying edit war guy constantly ...      0   \n",
       "3  0001b41b1c6bb37e  make real suggestions improvement wondered sec...      0   \n",
       "4  0001d958c54c6e35                      sir hero chance remember page      0   \n",
       "5  00025465d4725e87           congratulations well use tools well talk      0   \n",
       "6  0002bcb3da6cb337                        cocksucker piss around work      1   \n",
       "7  00031b1e95af7921  vandalism matt shirvington article reverted pl...      0   \n",
       "8  00037261f536c51d  sorry word nonsense offensive anyway intending...      0   \n",
       "9  00040093b2687caa               alignment subject contrary dulithgow      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "5             0        0       0       0              0   \n",
       "6             1        1       0       1              0   \n",
       "7             0        0       0       0              0   \n",
       "8             0        0       0       0              0   \n",
       "9             0        0       0       0              0   \n",
       "\n",
       "                                    tokenized_column  \n",
       "0  [explanation, edits, made, username, hardcore,...  \n",
       "1  [daww, matches, background, colour, seemingly,...  \n",
       "2  [hey, man, really, trying, edit, war, guy, con...  \n",
       "3  [make, real, suggestions, improvement, wondere...  \n",
       "4                [sir, hero, chance, remember, page]  \n",
       "5    [congratulations, well, use, tools, well, talk]  \n",
       "6                   [cocksucker, piss, around, work]  \n",
       "7  [vandalism, matt, shirvington, article, revert...  \n",
       "8  [sorry, word, nonsense, offensive, anyway, int...  \n",
       "9          [alignment, subject, contrary, dulithgow]  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Made a new column containing the tokenized data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To return the word to it's basic form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implementation of Lemmatization function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_tokens(tokens):\n",
    "    # initialize WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Lemmatize each word in the list of tokens\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # make the words into sentence instead of a list\n",
    "    return ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### apply function on tokenized column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function on tokenized column\n",
    "df['lemmatization_column']=df['tokenized_column'].apply(lemmatize_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>tokenized_column</th>\n",
       "      <th>lemmatization_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[explanation, edits, made, username, hardcore,...</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>daww matches background colour seemingly stuck...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[daww, matches, background, colour, seemingly,...</td>\n",
       "      <td>daww match background colour seemingly stuck t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[hey, man, really, trying, edit, war, guy, con...</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>make real suggestions improvement wondered sec...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[make, real, suggestions, improvement, wondere...</td>\n",
       "      <td>make real suggestion improvement wondered sect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[sir, hero, chance, remember, page]</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157664</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>second time asking view completely contradicts...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[second, time, asking, view, completely, contr...</td>\n",
       "      <td>second time asking view completely contradicts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157665</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>ashamed horrible thing put talk page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[ashamed, horrible, thing, put, talk, page]</td>\n",
       "      <td>ashamed horrible thing put talk page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157666</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>spitzer umm actual article prostitution ring c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[spitzer, umm, actual, article, prostitution, ...</td>\n",
       "      <td>spitzer umm actual article prostitution ring c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157667</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>looks like actually put speedy first version d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[looks, like, actually, put, speedy, first, ve...</td>\n",
       "      <td>look like actually put speedy first version de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157668</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>really think understand came idea bad right aw...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[really, think, understand, came, idea, bad, r...</td>\n",
       "      <td>really think understand came idea bad right aw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157669 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "0       0000997932d777bf  explanation edits made username hardcore metal...   \n",
       "1       000103f0d9cfb60f  daww matches background colour seemingly stuck...   \n",
       "2       000113f07ec002fd  hey man really trying edit war guy constantly ...   \n",
       "3       0001b41b1c6bb37e  make real suggestions improvement wondered sec...   \n",
       "4       0001d958c54c6e35                      sir hero chance remember page   \n",
       "...                  ...                                                ...   \n",
       "157664  ffe987279560d7ff  second time asking view completely contradicts...   \n",
       "157665  ffea4adeee384e90               ashamed horrible thing put talk page   \n",
       "157666  ffee36eab5c267c9  spitzer umm actual article prostitution ring c...   \n",
       "157667  fff125370e4aaaf3  looks like actually put speedy first version d...   \n",
       "157668  fff46fc426af1f9a  really think understand came idea bad right aw...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0           0             0        0       0       0              0   \n",
       "1           0             0        0       0       0              0   \n",
       "2           0             0        0       0       0              0   \n",
       "3           0             0        0       0       0              0   \n",
       "4           0             0        0       0       0              0   \n",
       "...       ...           ...      ...     ...     ...            ...   \n",
       "157664      0             0        0       0       0              0   \n",
       "157665      0             0        0       0       0              0   \n",
       "157666      0             0        0       0       0              0   \n",
       "157667      0             0        0       0       0              0   \n",
       "157668      0             0        0       0       0              0   \n",
       "\n",
       "                                         tokenized_column  \\\n",
       "0       [explanation, edits, made, username, hardcore,...   \n",
       "1       [daww, matches, background, colour, seemingly,...   \n",
       "2       [hey, man, really, trying, edit, war, guy, con...   \n",
       "3       [make, real, suggestions, improvement, wondere...   \n",
       "4                     [sir, hero, chance, remember, page]   \n",
       "...                                                   ...   \n",
       "157664  [second, time, asking, view, completely, contr...   \n",
       "157665        [ashamed, horrible, thing, put, talk, page]   \n",
       "157666  [spitzer, umm, actual, article, prostitution, ...   \n",
       "157667  [looks, like, actually, put, speedy, first, ve...   \n",
       "157668  [really, think, understand, came, idea, bad, r...   \n",
       "\n",
       "                                     lemmatization_column  \n",
       "0       explanation edits made username hardcore metal...  \n",
       "1       daww match background colour seemingly stuck t...  \n",
       "2       hey man really trying edit war guy constantly ...  \n",
       "3       make real suggestion improvement wondered sect...  \n",
       "4                           sir hero chance remember page  \n",
       "...                                                   ...  \n",
       "157664  second time asking view completely contradicts...  \n",
       "157665               ashamed horrible thing put talk page  \n",
       "157666  spitzer umm actual article prostitution ring c...  \n",
       "157667  look like actually put speedy first version de...  \n",
       "157668  really think understand came idea bad right aw...  \n",
       "\n",
       "[157669 rows x 10 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Made a new column containing the lemmatization data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Duplicates After lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization function will make some rows hold the same text , so will drop duplicantes on column lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply drop duplicates on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicates on lemmatization column\n",
    "df.drop_duplicates(subset='lemmatization_column',inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset index and drop column index that will be produced from reset index\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>tokenized_column</th>\n",
       "      <th>lemmatization_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[explanation, edits, made, username, hardcore,...</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>daww matches background colour seemingly stuck...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[daww, matches, background, colour, seemingly,...</td>\n",
       "      <td>daww match background colour seemingly stuck t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[hey, man, really, trying, edit, war, guy, con...</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>make real suggestions improvement wondered sec...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[make, real, suggestions, improvement, wondere...</td>\n",
       "      <td>make real suggestion improvement wondered sect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[sir, hero, chance, remember, page]</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157627</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>second time asking view completely contradicts...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[second, time, asking, view, completely, contr...</td>\n",
       "      <td>second time asking view completely contradicts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157628</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>ashamed horrible thing put talk page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[ashamed, horrible, thing, put, talk, page]</td>\n",
       "      <td>ashamed horrible thing put talk page</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157629</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>spitzer umm actual article prostitution ring c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[spitzer, umm, actual, article, prostitution, ...</td>\n",
       "      <td>spitzer umm actual article prostitution ring c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157630</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>looks like actually put speedy first version d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[looks, like, actually, put, speedy, first, ve...</td>\n",
       "      <td>look like actually put speedy first version de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157631</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>really think understand came idea bad right aw...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[really, think, understand, came, idea, bad, r...</td>\n",
       "      <td>really think understand came idea bad right aw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157632 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "0       0000997932d777bf  explanation edits made username hardcore metal...   \n",
       "1       000103f0d9cfb60f  daww matches background colour seemingly stuck...   \n",
       "2       000113f07ec002fd  hey man really trying edit war guy constantly ...   \n",
       "3       0001b41b1c6bb37e  make real suggestions improvement wondered sec...   \n",
       "4       0001d958c54c6e35                      sir hero chance remember page   \n",
       "...                  ...                                                ...   \n",
       "157627  ffe987279560d7ff  second time asking view completely contradicts...   \n",
       "157628  ffea4adeee384e90               ashamed horrible thing put talk page   \n",
       "157629  ffee36eab5c267c9  spitzer umm actual article prostitution ring c...   \n",
       "157630  fff125370e4aaaf3  looks like actually put speedy first version d...   \n",
       "157631  fff46fc426af1f9a  really think understand came idea bad right aw...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0           0             0        0       0       0              0   \n",
       "1           0             0        0       0       0              0   \n",
       "2           0             0        0       0       0              0   \n",
       "3           0             0        0       0       0              0   \n",
       "4           0             0        0       0       0              0   \n",
       "...       ...           ...      ...     ...     ...            ...   \n",
       "157627      0             0        0       0       0              0   \n",
       "157628      0             0        0       0       0              0   \n",
       "157629      0             0        0       0       0              0   \n",
       "157630      0             0        0       0       0              0   \n",
       "157631      0             0        0       0       0              0   \n",
       "\n",
       "                                         tokenized_column  \\\n",
       "0       [explanation, edits, made, username, hardcore,...   \n",
       "1       [daww, matches, background, colour, seemingly,...   \n",
       "2       [hey, man, really, trying, edit, war, guy, con...   \n",
       "3       [make, real, suggestions, improvement, wondere...   \n",
       "4                     [sir, hero, chance, remember, page]   \n",
       "...                                                   ...   \n",
       "157627  [second, time, asking, view, completely, contr...   \n",
       "157628        [ashamed, horrible, thing, put, talk, page]   \n",
       "157629  [spitzer, umm, actual, article, prostitution, ...   \n",
       "157630  [looks, like, actually, put, speedy, first, ve...   \n",
       "157631  [really, think, understand, came, idea, bad, r...   \n",
       "\n",
       "                                     lemmatization_column  \n",
       "0       explanation edits made username hardcore metal...  \n",
       "1       daww match background colour seemingly stuck t...  \n",
       "2       hey man really trying edit war guy constantly ...  \n",
       "3       make real suggestion improvement wondered sect...  \n",
       "4                           sir hero chance remember page  \n",
       "...                                                   ...  \n",
       "157627  second time asking view completely contradicts...  \n",
       "157628               ashamed horrible thing put talk page  \n",
       "157629  spitzer umm actual article prostitution ring c...  \n",
       "157630  look like actually put speedy first version de...  \n",
       "157631  really think understand came idea bad right aw...  \n",
       "\n",
       "[157632 rows x 10 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### See the number of rows has reduced from 157669 to 157632 , so there was duplicantes in column lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New column is made containing binary if it has offensive comment or not ; column called offensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### made offensive column because it is more practical to have binary classification model that tells you this is offensive or non-offensive comment for real life applications ; also to help solving imbalace of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This column will be used in classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>tokenized_column</th>\n",
       "      <th>lemmatization_column</th>\n",
       "      <th>offensive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[explanation, edits, made, username, hardcore,...</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>daww matches background colour seemingly stuck...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[daww, matches, background, colour, seemingly,...</td>\n",
       "      <td>daww match background colour seemingly stuck t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[hey, man, really, trying, edit, war, guy, con...</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>make real suggestions improvement wondered sec...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[make, real, suggestions, improvement, wondere...</td>\n",
       "      <td>make real suggestion improvement wondered sect...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[sir, hero, chance, remember, page]</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157627</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>second time asking view completely contradicts...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[second, time, asking, view, completely, contr...</td>\n",
       "      <td>second time asking view completely contradicts...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157628</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>ashamed horrible thing put talk page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[ashamed, horrible, thing, put, talk, page]</td>\n",
       "      <td>ashamed horrible thing put talk page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157629</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>spitzer umm actual article prostitution ring c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[spitzer, umm, actual, article, prostitution, ...</td>\n",
       "      <td>spitzer umm actual article prostitution ring c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157630</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>looks like actually put speedy first version d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[looks, like, actually, put, speedy, first, ve...</td>\n",
       "      <td>look like actually put speedy first version de...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157631</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>really think understand came idea bad right aw...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[really, think, understand, came, idea, bad, r...</td>\n",
       "      <td>really think understand came idea bad right aw...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157632 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "0       0000997932d777bf  explanation edits made username hardcore metal...   \n",
       "1       000103f0d9cfb60f  daww matches background colour seemingly stuck...   \n",
       "2       000113f07ec002fd  hey man really trying edit war guy constantly ...   \n",
       "3       0001b41b1c6bb37e  make real suggestions improvement wondered sec...   \n",
       "4       0001d958c54c6e35                      sir hero chance remember page   \n",
       "...                  ...                                                ...   \n",
       "157627  ffe987279560d7ff  second time asking view completely contradicts...   \n",
       "157628  ffea4adeee384e90               ashamed horrible thing put talk page   \n",
       "157629  ffee36eab5c267c9  spitzer umm actual article prostitution ring c...   \n",
       "157630  fff125370e4aaaf3  looks like actually put speedy first version d...   \n",
       "157631  fff46fc426af1f9a  really think understand came idea bad right aw...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0           0             0        0       0       0              0   \n",
       "1           0             0        0       0       0              0   \n",
       "2           0             0        0       0       0              0   \n",
       "3           0             0        0       0       0              0   \n",
       "4           0             0        0       0       0              0   \n",
       "...       ...           ...      ...     ...     ...            ...   \n",
       "157627      0             0        0       0       0              0   \n",
       "157628      0             0        0       0       0              0   \n",
       "157629      0             0        0       0       0              0   \n",
       "157630      0             0        0       0       0              0   \n",
       "157631      0             0        0       0       0              0   \n",
       "\n",
       "                                         tokenized_column  \\\n",
       "0       [explanation, edits, made, username, hardcore,...   \n",
       "1       [daww, matches, background, colour, seemingly,...   \n",
       "2       [hey, man, really, trying, edit, war, guy, con...   \n",
       "3       [make, real, suggestions, improvement, wondere...   \n",
       "4                     [sir, hero, chance, remember, page]   \n",
       "...                                                   ...   \n",
       "157627  [second, time, asking, view, completely, contr...   \n",
       "157628        [ashamed, horrible, thing, put, talk, page]   \n",
       "157629  [spitzer, umm, actual, article, prostitution, ...   \n",
       "157630  [looks, like, actually, put, speedy, first, ve...   \n",
       "157631  [really, think, understand, came, idea, bad, r...   \n",
       "\n",
       "                                     lemmatization_column  offensive  \n",
       "0       explanation edits made username hardcore metal...          0  \n",
       "1       daww match background colour seemingly stuck t...          0  \n",
       "2       hey man really trying edit war guy constantly ...          0  \n",
       "3       make real suggestion improvement wondered sect...          0  \n",
       "4                           sir hero chance remember page          0  \n",
       "...                                                   ...        ...  \n",
       "157627  second time asking view completely contradicts...          0  \n",
       "157628               ashamed horrible thing put talk page          0  \n",
       "157629  spitzer umm actual article prostitution ring c...          0  \n",
       "157630  look like actually put speedy first version de...          0  \n",
       "157631  really think understand came idea bad right aw...          0  \n",
       "\n",
       "[157632 rows x 11 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new column with make value equal 1\n",
    "df['offensive'] = 1\n",
    "\n",
    "#codition is the row doesnot classify as toxic nor severe_toxic nor obscene nor threat nor insult nor identity_hate\n",
    "condition = (df[\"toxic\"]==0)&(df[\"severe_toxic\"]==0)&(df[\"obscene\"]==0)&(df[\"threat\"]==0)&(df[\"insult\"]==0)&(df[\"identity_hate\"]==0)\n",
    "\n",
    "# if it meet the condition and doesnot has offensive comment make the value equal 0\n",
    "df.loc[condition, 'offensive'] = 0\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A new column called offensive is made "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make two dictionary to make each unique word with an unique index\n",
    "- token_to_index : the word is the index\n",
    "- index_to_token : the number is the index \n",
    "### There is another dictionary made \n",
    "- vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function takes column name and data\n",
    "def create_word_vocabulary_and_index(dataset,column_name):\n",
    "    # initialize the set \n",
    "    vocabulary = set()\n",
    "    # Start with [PAD] as index 0 in token_to_index \n",
    "    # because when make any padding which equal to zero the PAD holds the same value\n",
    "    token_to_index = {'[PAD]': 0}\n",
    "    # Start with 0 as index [PAD] in index_to_token\n",
    "    index_to_token = {0: '[PAD]'}\n",
    "\n",
    "    # Start the index = 1\n",
    "    current_index = 1  \n",
    "    # iterate over each row in data on a specific column\n",
    "    for text in dataset[column_name]:\n",
    "        # split data into word\n",
    "        tokens = text.split()\n",
    "        # iterate over each word\n",
    "        for token in tokens:\n",
    "            # if the word appears for the first time in token_to_index dictionary\n",
    "            if token not in token_to_index:\n",
    "                # token_to_index with index word = the current index\n",
    "                token_to_index[token] = current_index\n",
    "                # index_to_token with index current_index = the current word\n",
    "                index_to_token[current_index] = token\n",
    "                # add +1 to index\n",
    "                current_index += 1\n",
    "        # update the tokens in the vocabulary\n",
    "        vocabulary.update(tokens)\n",
    "\n",
    "    return token_to_index, index_to_token , vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply function on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply function on lemmatization column\n",
    "token_to_index, index_to_token , vocabulary = create_word_vocabulary_and_index(df, 'lemmatization_column')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[PAD]': 0,\n",
       " 'explanation': 1,\n",
       " 'edits': 2,\n",
       " 'made': 3,\n",
       " 'username': 4,\n",
       " 'hardcore': 5,\n",
       " 'metallica': 6,\n",
       " 'fan': 7,\n",
       " 'reverted': 8,\n",
       " 'vandalism': 9,\n",
       " 'closure': 10,\n",
       " 'gas': 11,\n",
       " 'voted': 12,\n",
       " 'new': 13,\n",
       " 'york': 14,\n",
       " 'doll': 15,\n",
       " 'fac': 16,\n",
       " 'please': 17,\n",
       " 'remove': 18,\n",
       " 'template': 19,\n",
       " 'talk': 20,\n",
       " 'page': 21,\n",
       " 'since': 22,\n",
       " 'retired': 23,\n",
       " 'daww': 24,\n",
       " 'match': 25,\n",
       " 'background': 26,\n",
       " 'colour': 27,\n",
       " 'seemingly': 28,\n",
       " 'stuck': 29,\n",
       " 'thanks': 30,\n",
       " 'january': 31,\n",
       " 'utc': 32,\n",
       " 'hey': 33,\n",
       " 'man': 34,\n",
       " 'really': 35,\n",
       " 'trying': 36,\n",
       " 'edit': 37,\n",
       " 'war': 38,\n",
       " 'guy': 39,\n",
       " 'constantly': 40,\n",
       " 'removing': 41,\n",
       " 'relevant': 42,\n",
       " 'information': 43,\n",
       " 'talking': 44,\n",
       " 'instead': 45,\n",
       " 'seems': 46,\n",
       " 'care': 47,\n",
       " 'formatting': 48,\n",
       " 'actual': 49,\n",
       " 'info': 50,\n",
       " 'make': 51,\n",
       " 'real': 52,\n",
       " 'suggestion': 53,\n",
       " 'improvement': 54,\n",
       " 'wondered': 55,\n",
       " 'section': 56,\n",
       " 'statistic': 57,\n",
       " 'later': 58,\n",
       " 'subsection': 59,\n",
       " 'type': 60,\n",
       " 'accident': 61,\n",
       " 'think': 62,\n",
       " 'reference': 63,\n",
       " 'may': 64,\n",
       " 'need': 65,\n",
       " 'tidying': 66,\n",
       " 'exact': 67,\n",
       " 'format': 68,\n",
       " 'ie': 69,\n",
       " 'date': 70,\n",
       " 'etc': 71,\n",
       " 'noone': 72,\n",
       " 'else': 73,\n",
       " 'first': 74,\n",
       " 'preference': 75,\n",
       " 'style': 76,\n",
       " 'want': 77,\n",
       " 'let': 78,\n",
       " 'know': 79,\n",
       " 'appears': 80,\n",
       " 'backlog': 81,\n",
       " 'article': 82,\n",
       " 'review': 83,\n",
       " 'guess': 84,\n",
       " 'delay': 85,\n",
       " 'reviewer': 86,\n",
       " 'turn': 87,\n",
       " 'listed': 88,\n",
       " 'form': 89,\n",
       " 'eg': 90,\n",
       " 'wikipediagoodarticlenominationstransport': 91,\n",
       " 'sir': 92,\n",
       " 'hero': 93,\n",
       " 'chance': 94,\n",
       " 'remember': 95,\n",
       " 'congratulation': 96,\n",
       " 'well': 97,\n",
       " 'use': 98,\n",
       " 'tool': 99,\n",
       " 'cocksucker': 100,\n",
       " 'piss': 101,\n",
       " 'around': 102,\n",
       " 'work': 103,\n",
       " 'matt': 104,\n",
       " 'shirvington': 105,\n",
       " 'banned': 106,\n",
       " 'sorry': 107,\n",
       " 'word': 108,\n",
       " 'nonsense': 109,\n",
       " 'offensive': 110,\n",
       " 'anyway': 111,\n",
       " 'intending': 112,\n",
       " 'write': 113,\n",
       " 'anything': 114,\n",
       " 'articlewow': 115,\n",
       " 'would': 116,\n",
       " 'jump': 117,\n",
       " 'merely': 118,\n",
       " 'requesting': 119,\n",
       " 'encyclopedic': 120,\n",
       " 'one': 121,\n",
       " 'school': 122,\n",
       " 'selective': 123,\n",
       " 'breeding': 124,\n",
       " 'almost': 125,\n",
       " 'stub': 126,\n",
       " 'point': 127,\n",
       " 'animal': 128,\n",
       " 'short': 129,\n",
       " 'messy': 130,\n",
       " 'give': 131,\n",
       " 'must': 132,\n",
       " 'someone': 133,\n",
       " 'expertise': 134,\n",
       " 'eugenics': 135,\n",
       " 'alignment': 136,\n",
       " 'subject': 137,\n",
       " 'contrary': 138,\n",
       " 'dulithgow': 139,\n",
       " 'fair': 140,\n",
       " 'rationale': 141,\n",
       " 'imagewonjujpg': 142,\n",
       " 'uploading': 143,\n",
       " 'notice': 144,\n",
       " 'image': 145,\n",
       " 'specifies': 146,\n",
       " 'used': 147,\n",
       " 'wikipedia': 148,\n",
       " 'constitutes': 149,\n",
       " 'addition': 150,\n",
       " 'boilerplate': 151,\n",
       " 'also': 152,\n",
       " 'description': 153,\n",
       " 'specific': 154,\n",
       " 'using': 155,\n",
       " 'consistent': 156,\n",
       " 'go': 157,\n",
       " 'include': 158,\n",
       " 'uploaded': 159,\n",
       " 'medium': 160,\n",
       " 'consider': 161,\n",
       " 'checking': 162,\n",
       " 'specified': 163,\n",
       " 'find': 164,\n",
       " 'list': 165,\n",
       " 'edited': 166,\n",
       " 'clicking': 167,\n",
       " 'contribution': 168,\n",
       " 'link': 169,\n",
       " 'located': 170,\n",
       " 'top': 171,\n",
       " 'logged': 172,\n",
       " 'selecting': 173,\n",
       " 'dropdown': 174,\n",
       " 'box': 175,\n",
       " 'note': 176,\n",
       " 'lacking': 177,\n",
       " 'deleted': 178,\n",
       " 'week': 179,\n",
       " 'described': 180,\n",
       " 'criterion': 181,\n",
       " 'speedy': 182,\n",
       " 'deletion': 183,\n",
       " 'question': 184,\n",
       " 'ask': 185,\n",
       " 'copyright': 186,\n",
       " 'thank': 187,\n",
       " 'contribs': 188,\n",
       " 'unspecified': 189,\n",
       " 'source': 190,\n",
       " 'noticed': 191,\n",
       " 'file': 192,\n",
       " 'currently': 193,\n",
       " 'specify': 194,\n",
       " 'created': 195,\n",
       " 'content': 196,\n",
       " 'status': 197,\n",
       " 'unclear': 198,\n",
       " 'create': 199,\n",
       " 'owner': 200,\n",
       " 'obtained': 201,\n",
       " 'website': 202,\n",
       " 'taken': 203,\n",
       " 'together': 204,\n",
       " 'restatement': 205,\n",
       " 'term': 206,\n",
       " 'usually': 207,\n",
       " 'sufficient': 208,\n",
       " 'however': 209,\n",
       " 'holder': 210,\n",
       " 'different': 211,\n",
       " 'publisher': 212,\n",
       " 'acknowledged': 213,\n",
       " 'adding': 214,\n",
       " 'add': 215,\n",
       " 'proper': 216,\n",
       " 'licensing': 217,\n",
       " 'tag': 218,\n",
       " 'already': 219,\n",
       " 'createdtook': 220,\n",
       " 'picture': 221,\n",
       " 'audio': 222,\n",
       " 'video': 223,\n",
       " 'release': 224,\n",
       " 'gfdl': 225,\n",
       " 'believe': 226,\n",
       " 'meet': 227,\n",
       " 'wikipediafair': 228,\n",
       " 'wikipediaimage': 229,\n",
       " 'tagsfair': 230,\n",
       " 'see': 231,\n",
       " 'full': 232,\n",
       " 'tagged': 233,\n",
       " 'following': 234,\n",
       " 'unsourced': 235,\n",
       " 'untagged': 236,\n",
       " 'copyrighted': 237,\n",
       " 'nonfree': 238,\n",
       " 'license': 239,\n",
       " 'per': 240,\n",
       " 'hour': 241,\n",
       " 'bbq': 242,\n",
       " 'u': 243,\n",
       " 'discus': 244,\n",
       " 'itmaybe': 245,\n",
       " 'phone': 246,\n",
       " 'exclusive': 247,\n",
       " 'group': 248,\n",
       " 'wp': 249,\n",
       " 'talibanswho': 250,\n",
       " 'good': 251,\n",
       " 'destroying': 252,\n",
       " 'selfappointed': 253,\n",
       " 'purist': 254,\n",
       " 'gang': 255,\n",
       " 'asks': 256,\n",
       " 'antisocial': 257,\n",
       " 'destructive': 258,\n",
       " 'noncontribution': 259,\n",
       " 'sityush': 260,\n",
       " 'clean': 261,\n",
       " 'behavior': 262,\n",
       " 'issue': 263,\n",
       " 'nonsensical': 264,\n",
       " 'warning': 265,\n",
       " 'start': 266,\n",
       " 'throwing': 267,\n",
       " 'accusation': 268,\n",
       " 'itselfmaking': 269,\n",
       " 'ad': 270,\n",
       " 'hominem': 271,\n",
       " 'attack': 272,\n",
       " 'going': 273,\n",
       " 'strengthen': 274,\n",
       " 'argument': 275,\n",
       " 'look': 276,\n",
       " 'like': 277,\n",
       " 'abusing': 278,\n",
       " 'power': 279,\n",
       " 'admin': 280,\n",
       " 'relevantthis': 281,\n",
       " 'probably': 282,\n",
       " 'single': 283,\n",
       " 'talked': 284,\n",
       " 'event': 285,\n",
       " 'int': 286,\n",
       " 'news': 287,\n",
       " 'late': 288,\n",
       " 'absence': 289,\n",
       " 'notable': 290,\n",
       " 'living': 291,\n",
       " 'expresident': 292,\n",
       " 'attend': 293,\n",
       " 'certainly': 294,\n",
       " 'dedicating': 295,\n",
       " 'aircracft': 296,\n",
       " 'carrier': 297,\n",
       " 'intend': 298,\n",
       " 'revert': 299,\n",
       " 'hope': 300,\n",
       " 'attracting': 301,\n",
       " 'attention': 302,\n",
       " 'willing': 303,\n",
       " 'throw': 304,\n",
       " 'quite': 305,\n",
       " 'liberally': 306,\n",
       " 'perhaps': 307,\n",
       " 'achieve': 308,\n",
       " 'level': 309,\n",
       " 'civility': 310,\n",
       " 'rational': 311,\n",
       " 'discussion': 312,\n",
       " 'topic': 313,\n",
       " 'resolve': 314,\n",
       " 'matter': 315,\n",
       " 'peacefully': 316,\n",
       " 'oh': 317,\n",
       " 'girl': 318,\n",
       " 'started': 319,\n",
       " 'nose': 320,\n",
       " 'belong': 321,\n",
       " 'yvesnimmo': 322,\n",
       " 'said': 323,\n",
       " 'situation': 324,\n",
       " 'settled': 325,\n",
       " 'apologized': 326,\n",
       " 'juelz': 327,\n",
       " 'santanas': 328,\n",
       " 'age': 329,\n",
       " 'santana': 330,\n",
       " 'year': 331,\n",
       " 'old': 332,\n",
       " 'came': 333,\n",
       " 'february': 334,\n",
       " 'th': 335,\n",
       " 'making': 336,\n",
       " 'song': 337,\n",
       " 'diplomat': 338,\n",
       " 'third': 339,\n",
       " 'neff': 340,\n",
       " 'signed': 341,\n",
       " 'cam': 342,\n",
       " 'label': 343,\n",
       " 'roc': 344,\n",
       " 'fella': 345,\n",
       " 'coming': 346,\n",
       " 'town': 347,\n",
       " 'yes': 348,\n",
       " 'born': 349,\n",
       " 'could': 350,\n",
       " 'older': 351,\n",
       " 'lloyd': 352,\n",
       " 'bank': 353,\n",
       " 'birthday': 354,\n",
       " 'passed': 355,\n",
       " 'homie': 356,\n",
       " 'death': 357,\n",
       " 'god': 358,\n",
       " 'forbid': 359,\n",
       " 'thinking': 360,\n",
       " 'equal': 361,\n",
       " 'caculator': 362,\n",
       " 'stop': 363,\n",
       " 'changing': 364,\n",
       " 'birth': 365,\n",
       " 'bye': 366,\n",
       " 'come': 367,\n",
       " 'comming': 368,\n",
       " 'back': 369,\n",
       " 'tosser': 370,\n",
       " 'redirect': 371,\n",
       " 'talkvoydan': 372,\n",
       " 'pop': 373,\n",
       " 'georgiev': 374,\n",
       " 'chernodrinski': 375,\n",
       " 'mitsurugi': 376,\n",
       " 'sense': 377,\n",
       " 'argue': 378,\n",
       " 'hindi': 379,\n",
       " 'ryo': 380,\n",
       " 'sakazakis': 381,\n",
       " 'mean': 382,\n",
       " 'bother': 383,\n",
       " 'writing': 384,\n",
       " 'something': 385,\n",
       " 'regarding': 386,\n",
       " 'posted': 387,\n",
       " 'acctually': 388,\n",
       " 'even': 389,\n",
       " 'better': 390,\n",
       " 'take': 391,\n",
       " 'closer': 392,\n",
       " 'premature': 393,\n",
       " 'wrestling': 394,\n",
       " 'catagory': 395,\n",
       " 'men': 396,\n",
       " 'surely': 397,\n",
       " 'besides': 398,\n",
       " 'delting': 399,\n",
       " 'recent': 400,\n",
       " 'read': 401,\n",
       " 'wpfilmplot': 402,\n",
       " 'editing': 403,\n",
       " 'film': 404,\n",
       " 'simply': 405,\n",
       " 'entirely': 406,\n",
       " 'many': 407,\n",
       " 'unnecessary': 408,\n",
       " 'detail': 409,\n",
       " 'bad': 410,\n",
       " 'damage': 411,\n",
       " 'yeah': 412,\n",
       " 'studying': 413,\n",
       " 'nowdeepu': 414,\n",
       " 'snowflake': 415,\n",
       " 'always': 416,\n",
       " 'symmetrical': 417,\n",
       " 'geometry': 418,\n",
       " 'stated': 419,\n",
       " 'six': 420,\n",
       " 'symmetric': 421,\n",
       " 'arm': 422,\n",
       " 'assertion': 423,\n",
       " 'true': 424,\n",
       " 'according': 425,\n",
       " 'kenneth': 426,\n",
       " 'libbrecht': 427,\n",
       " 'rather': 428,\n",
       " 'unattractive': 429,\n",
       " 'irregular': 430,\n",
       " 'crystal': 431,\n",
       " 'far': 432,\n",
       " 'common': 433,\n",
       " 'variety': 434,\n",
       " 'site': 435,\n",
       " 'get': 436,\n",
       " 'fact': 437,\n",
       " 'still': 438,\n",
       " 'decent': 439,\n",
       " 'number': 440,\n",
       " 'falsity': 441,\n",
       " 'forgive': 442,\n",
       " 'signpost': 443,\n",
       " 'september': 444,\n",
       " 'singlepage': 445,\n",
       " 'unsubscribe': 446,\n",
       " 'reconsidering': 447,\n",
       " 'st': 448,\n",
       " 'paragraph': 449,\n",
       " 'understand': 450,\n",
       " 'reason': 451,\n",
       " 'sure': 452,\n",
       " 'data': 453,\n",
       " 'necessarily': 454,\n",
       " 'wrong': 455,\n",
       " 'persuaded': 456,\n",
       " 'strategy': 457,\n",
       " 'introducing': 458,\n",
       " 'academic': 459,\n",
       " 'honor': 460,\n",
       " 'unhelpful': 461,\n",
       " 'approach': 462,\n",
       " 'sitting': 463,\n",
       " 'justice': 464,\n",
       " 'similarly': 465,\n",
       " 'enhanced': 466,\n",
       " 'change': 467,\n",
       " 'support': 468,\n",
       " 'view': 469,\n",
       " 'invite': 470,\n",
       " 'anyone': 471,\n",
       " 'revisit': 472,\n",
       " 'written': 473,\n",
       " 'pair': 474,\n",
       " 'jurist': 475,\n",
       " 'benjamin': 476,\n",
       " 'cardozo': 477,\n",
       " 'learned': 478,\n",
       " 'hand': 479,\n",
       " 'b': 480,\n",
       " 'john': 481,\n",
       " 'marshall': 482,\n",
       " 'harlan': 483,\n",
       " 'ii': 484,\n",
       " 'becomes': 485,\n",
       " 'current': 486,\n",
       " 'version': 487,\n",
       " 'either': 488,\n",
       " 'improved': 489,\n",
       " 'credential': 490,\n",
       " 'introductory': 491,\n",
       " 'help': 492,\n",
       " 'repeat': 493,\n",
       " 'wry': 494,\n",
       " 'kathleen': 495,\n",
       " 'sullivan': 496,\n",
       " 'stanford': 497,\n",
       " 'law': 498,\n",
       " 'suggests': 499,\n",
       " 'harvard': 500,\n",
       " 'faculty': 501,\n",
       " 'wonder': 502,\n",
       " 'antonin': 503,\n",
       " 'scalia': 504,\n",
       " 'avoided': 505,\n",
       " 'learning': 506,\n",
       " 'others': 507,\n",
       " 'managed': 508,\n",
       " 'grasp': 509,\n",
       " 'process': 510,\n",
       " 'judging': 511,\n",
       " 'anecdote': 512,\n",
       " 'gently': 513,\n",
       " 'illustrates': 514,\n",
       " 'le': 515,\n",
       " 'humorous': 516,\n",
       " 'stronger': 517,\n",
       " 'clarence': 518,\n",
       " 'thomas': 519,\n",
       " 'mention': 520,\n",
       " 'wanting': 521,\n",
       " 'return': 522,\n",
       " 'degree': 523,\n",
       " 'yale': 524,\n",
       " 'minimum': 525,\n",
       " 'questioning': 526,\n",
       " 'deserves': 527,\n",
       " 'reconsidered': 528,\n",
       " 'radial': 529,\n",
       " 'symmetry': 530,\n",
       " 'several': 531,\n",
       " 'extinct': 532,\n",
       " 'lineage': 533,\n",
       " 'included': 534,\n",
       " 'echinodermata': 535,\n",
       " 'bilateral': 536,\n",
       " 'homostelea': 537,\n",
       " 'asymmetrical': 538,\n",
       " 'cothurnocystis': 539,\n",
       " 'stylophora': 540,\n",
       " 'apologize': 541,\n",
       " 'reconciling': 542,\n",
       " 'knowledge': 543,\n",
       " 'done': 544,\n",
       " 'history': 545,\n",
       " 'study': 546,\n",
       " 'archaeology': 547,\n",
       " 'scan': 548,\n",
       " 'email': 549,\n",
       " 'translate': 550,\n",
       " 'mother': 551,\n",
       " 'child': 552,\n",
       " 'case': 553,\n",
       " 'michael': 554,\n",
       " 'jackson': 555,\n",
       " 'studied': 556,\n",
       " 'motif': 557,\n",
       " 'reasoning': 558,\n",
       " 'judged': 559,\n",
       " 'upon': 560,\n",
       " 'character': 561,\n",
       " 'harshly': 562,\n",
       " 'wacko': 563,\n",
       " 'jacko': 564,\n",
       " 'tell': 565,\n",
       " 'ignore': 566,\n",
       " 'incriminate': 567,\n",
       " 'continue': 568,\n",
       " 'refuting': 569,\n",
       " 'bullshit': 570,\n",
       " 'jayjg': 571,\n",
       " 'keep': 572,\n",
       " 'jun': 573,\n",
       " 'ok': 574,\n",
       " 'bit': 575,\n",
       " 'example': 576,\n",
       " 'base': 577,\n",
       " 'duck': 578,\n",
       " 'barnstar': 579,\n",
       " 'life': 580,\n",
       " 'star': 581,\n",
       " 'post': 582,\n",
       " 'block': 583,\n",
       " 'expires': 584,\n",
       " 'funny': 585,\n",
       " 'thing': 586,\n",
       " 'uncivil': 587,\n",
       " 'heading': 588,\n",
       " 'fight': 589,\n",
       " 'freedom': 590,\n",
       " 'contain': 591,\n",
       " 'praise': 592,\n",
       " 'looked': 593,\n",
       " 'month': 594,\n",
       " 'ago': 595,\n",
       " 'much': 596,\n",
       " 'able': 597,\n",
       " 'quickly': 598,\n",
       " 'text': 599,\n",
       " 'hard': 600,\n",
       " 'drive': 601,\n",
       " 'meaning': 602,\n",
       " 'updating': 603,\n",
       " 'sound': 604,\n",
       " 'time': 605,\n",
       " 'generating': 606,\n",
       " 'interest': 607,\n",
       " 'spent': 608,\n",
       " 'four': 609,\n",
       " 'drum': 610,\n",
       " 'freely': 611,\n",
       " 'licensed': 612,\n",
       " 'length': 613,\n",
       " 'classical': 614,\n",
       " 'music': 615,\n",
       " 'unfortunately': 616,\n",
       " 'attempt': 617,\n",
       " 'failed': 618,\n",
       " 'effectively': 619,\n",
       " 'wikiproject': 620,\n",
       " 'interested': 621,\n",
       " 'wikipediatalkwikiprojectclassicalmusicarchive': 622,\n",
       " 'needhelp': 623,\n",
       " 'wikipediatalkwikiprojectmusicarchive': 624,\n",
       " 'icouldusesomehelpwikipediatalkwikiprojectmusicarchive': 625,\n",
       " 'raulbot': 626,\n",
       " 'candthemusiclist': 627,\n",
       " 'given': 628,\n",
       " 'featured': 629,\n",
       " 'digg': 630,\n",
       " 'got': 631,\n",
       " 'diggs': 632,\n",
       " 'impressive': 633,\n",
       " 'subpages': 634,\n",
       " 'rfa': 635,\n",
       " 'noseptembers': 636,\n",
       " 'difference': 637,\n",
       " 'elc': 638,\n",
       " 'surprised': 639,\n",
       " 'left': 640,\n",
       " 'tc': 641,\n",
       " 'straw': 642,\n",
       " 'never': 643,\n",
       " 'claimed': 644,\n",
       " 'odonohue': 645,\n",
       " 'position': 646,\n",
       " 'practitioner': 647,\n",
       " 'researcher': 648,\n",
       " 'field': 649,\n",
       " 'ignored': 650,\n",
       " 'dsm': 651,\n",
       " 'exactly': 652,\n",
       " 'quote': 653,\n",
       " 'say': 654,\n",
       " 'agrees': 655,\n",
       " 'combating': 656,\n",
       " 'notion': 657,\n",
       " 'absurd': 658,\n",
       " 'part': 659,\n",
       " 'claim': 660,\n",
       " 'pedophilia': 661,\n",
       " 'sexual': 662,\n",
       " 'orientation': 663,\n",
       " 'hold': 664,\n",
       " 'unfair': 665,\n",
       " 'call': 666,\n",
       " 'disorder': 667,\n",
       " 'divided': 668,\n",
       " 'end': 669,\n",
       " 'day': 670,\n",
       " 'value': 671,\n",
       " 'judgment': 672,\n",
       " 'cantor': 673,\n",
       " 'pointed': 674,\n",
       " 'earlier': 675,\n",
       " 'thread': 676,\n",
       " 'scientific': 677,\n",
       " 'judgement': 678,\n",
       " 'choose': 679,\n",
       " 'clearly': 680,\n",
       " 'pretend': 681,\n",
       " 'basis': 682,\n",
       " 'mainland': 683,\n",
       " 'asia': 684,\n",
       " 'includes': 685,\n",
       " 'lower': 686,\n",
       " 'basin': 687,\n",
       " 'china': 688,\n",
       " 'yangtze': 689,\n",
       " 'river': 690,\n",
       " 'korea': 691,\n",
       " 'fine': 692,\n",
       " 'found': 693,\n",
       " 'citation': 694,\n",
       " 'comprehensive': 695,\n",
       " 'dna': 696,\n",
       " 'hammer': 697,\n",
       " 'generarizations': 698,\n",
       " 'speculation': 699,\n",
       " 'yayoi': 700,\n",
       " 'culture': 701,\n",
       " 'brought': 702,\n",
       " 'japan': 703,\n",
       " 'migrant': 704,\n",
       " 'trace': 705,\n",
       " 'root': 706,\n",
       " 'southeast': 707,\n",
       " 'asiasouth': 708,\n",
       " 'describes': 709,\n",
       " 'migration': 710,\n",
       " 'based': 711,\n",
       " 'osry': 712,\n",
       " 'gene': 713,\n",
       " 'close': 714,\n",
       " 'haplogroups': 715,\n",
       " 'om': 716,\n",
       " 'reiterates': 717,\n",
       " 'entire': 718,\n",
       " 'haplogroup': 719,\n",
       " 'proposed': 720,\n",
       " 'asian': 721,\n",
       " 'origin': 722,\n",
       " 'definition': 723,\n",
       " 'southern': 724,\n",
       " 'hypothesizes': 725,\n",
       " 'dispersal': 726,\n",
       " 'neolithic': 727,\n",
       " 'farmer': 728,\n",
       " 'eventually': 729,\n",
       " 'concluding': 730,\n",
       " 'state': 731,\n",
       " 'propose': 732,\n",
       " 'chromosome': 733,\n",
       " 'descend': 734,\n",
       " 'prehistoric': 735,\n",
       " 'southeastern': 736,\n",
       " 'agriculture': 737,\n",
       " 'region': 738,\n",
       " 'global': 739,\n",
       " 'sample': 740,\n",
       " 'consisted': 741,\n",
       " 'male': 742,\n",
       " 'population': 743,\n",
       " 'including': 744,\n",
       " 'sampled': 745,\n",
       " 'across': 746,\n",
       " 'japanese': 747,\n",
       " 'archipelago': 748,\n",
       " 'pretty': 749,\n",
       " 'everyone': 750,\n",
       " 'warren': 751,\n",
       " 'countysurrounding': 752,\n",
       " 'glen': 753,\n",
       " 'fall': 754,\n",
       " 'hospital': 755,\n",
       " 'qualifies': 756,\n",
       " 'native': 757,\n",
       " 'rachel': 758,\n",
       " 'ray': 759,\n",
       " 'actually': 760,\n",
       " 'lake': 761,\n",
       " 'luzerne': 762,\n",
       " 'preceding': 763,\n",
       " 'unsigned': 764,\n",
       " 'comment': 765,\n",
       " 'added': 766,\n",
       " 'august': 767,\n",
       " 'hi': 768,\n",
       " 'explicit': 769,\n",
       " 'fenian': 770,\n",
       " 'editwarring': 771,\n",
       " 'giant': 772,\n",
       " 'causeway': 773,\n",
       " 'terrorism': 774,\n",
       " 'notability': 775,\n",
       " 'rurika': 776,\n",
       " 'kasuga': 777,\n",
       " 'placed': 778,\n",
       " 'speedily': 779,\n",
       " 'person': 780,\n",
       " 'people': 781,\n",
       " 'band': 782,\n",
       " 'club': 783,\n",
       " 'company': 784,\n",
       " 'web': 785,\n",
       " 'indicate': 786,\n",
       " 'assert': 787,\n",
       " 'guideline': 788,\n",
       " 'generally': 789,\n",
       " 'accepted': 790,\n",
       " 'contest': 791,\n",
       " 'tagging': 792,\n",
       " 'existing': 793,\n",
       " 'db': 794,\n",
       " 'leave': 795,\n",
       " 'explaining': 796,\n",
       " 'hesitate': 797,\n",
       " 'confirm': 798,\n",
       " 'check': 799,\n",
       " 'biography': 800,\n",
       " 'feel': 801,\n",
       " 'free': 802,\n",
       " 'lead': 803,\n",
       " 'briefly': 804,\n",
       " 'summarize': 805,\n",
       " 'armenia': 806,\n",
       " 'necessary': 807,\n",
       " 'sentence': 808,\n",
       " 'redundant': 809,\n",
       " 'welcome': 810,\n",
       " 'tfd': 811,\n",
       " 'eced': 812,\n",
       " 'responded': 813,\n",
       " 'without': 814,\n",
       " 'seeing': 815,\n",
       " 'response': 816,\n",
       " 'saw': 817,\n",
       " 'mine': 818,\n",
       " 'tcwpchicagowpfour': 819,\n",
       " 'gay': 820,\n",
       " 'antisemmitian': 821,\n",
       " 'archangel': 822,\n",
       " 'white': 823,\n",
       " 'tiger': 824,\n",
       " 'meow': 825,\n",
       " 'greetingshhh': 826,\n",
       " 'uh': 827,\n",
       " 'two': 828,\n",
       " 'way': 829,\n",
       " 'erased': 830,\n",
       " 'ww': 831,\n",
       " 'holocaust': 832,\n",
       " 'brutally': 833,\n",
       " 'slaying': 834,\n",
       " 'jew': 835,\n",
       " 'gaysgypsysslavsanyone': 836,\n",
       " 'antisemitian': 837,\n",
       " 'shave': 838,\n",
       " 'head': 839,\n",
       " 'bald': 840,\n",
       " 'skinhead': 841,\n",
       " 'meeting': 842,\n",
       " 'doubt': 843,\n",
       " 'bible': 844,\n",
       " 'homosexuality': 845,\n",
       " 'deadly': 846,\n",
       " 'sin': 847,\n",
       " 'pentagram': 848,\n",
       " 'tatoo': 849,\n",
       " 'forehead': 850,\n",
       " 'satanistic': 851,\n",
       " 'mass': 852,\n",
       " 'pal': 853,\n",
       " 'last': 854,\n",
       " 'fucking': 855,\n",
       " 'appreciate': 856,\n",
       " 'nazi': 857,\n",
       " 'shwain': 858,\n",
       " 'wish': 859,\n",
       " 'anymore': 860,\n",
       " 'beware': 861,\n",
       " 'dark': 862,\n",
       " 'side': 863,\n",
       " 'fuck': 864,\n",
       " 'filthy': 865,\n",
       " 'as': 866,\n",
       " 'dry': 867,\n",
       " 'screwed': 868,\n",
       " 'dominance': 869,\n",
       " 'bow': 870,\n",
       " 'almighty': 871,\n",
       " 'administrator': 872,\n",
       " 'play': 873,\n",
       " 'outsidewith': 874,\n",
       " 'mom': 875,\n",
       " 'lisak': 876,\n",
       " 'criticism': 877,\n",
       " 'present': 878,\n",
       " 'conforms': 879,\n",
       " 'npv': 880,\n",
       " 'rule': 881,\n",
       " 'neutral': 882,\n",
       " 'begin': 883,\n",
       " 'offer': 884,\n",
       " 'polygraph': 885,\n",
       " 'concerned': 886,\n",
       " 'result': 887,\n",
       " 'shock': 888,\n",
       " 'complainant': 889,\n",
       " 'lie': 890,\n",
       " 'uncovered': 891,\n",
       " 'recantation': 892,\n",
       " 'perfectly': 893,\n",
       " 'valid': 894,\n",
       " 'telling': 895,\n",
       " 'truth': 896,\n",
       " 'machine': 897,\n",
       " 'investigator': 898,\n",
       " 'kanins': 899,\n",
       " 'research': 900,\n",
       " 'followup': 901,\n",
       " 'recanted': 902,\n",
       " 'story': 903,\n",
       " 'possible': 904,\n",
       " 'verify': 905,\n",
       " 'false': 906,\n",
       " 'matched': 907,\n",
       " 'accused': 908,\n",
       " 'happened': 909,\n",
       " 'arguing': 910,\n",
       " 'respected': 911,\n",
       " 'phd': 912,\n",
       " 'baseless': 913,\n",
       " 'kanin': 914,\n",
       " 'agree': 915,\n",
       " 'though': 916,\n",
       " 'ammended': 917,\n",
       " 'appropriate': 918,\n",
       " 'notabilitysignificance': 919,\n",
       " 'lazy': 920,\n",
       " 'stalking': 921,\n",
       " 'absolute': 922,\n",
       " 'rubbish': 923,\n",
       " 'serf': 924,\n",
       " 'aggravate': 925,\n",
       " 'assumed': 926,\n",
       " 'faith': 927,\n",
       " 'intention': 928,\n",
       " 'suggested': 929,\n",
       " 'seen': 930,\n",
       " 'suggest': 931,\n",
       " 'might': 932,\n",
       " 'ulterior': 933,\n",
       " 'motive': 934,\n",
       " 'massadding': 935,\n",
       " 'ever': 936,\n",
       " 'administrative': 937,\n",
       " 'mentioned': 938,\n",
       " 'role': 939,\n",
       " 'party': 940,\n",
       " 'disagreement': 941,\n",
       " 'rate': 942,\n",
       " 'conflict': 943,\n",
       " 'thus': 944,\n",
       " 'extend': 945,\n",
       " 'toward': 946,\n",
       " 'spurious': 947,\n",
       " 'unfounded': 948,\n",
       " 'chatspy': 949,\n",
       " 'jmabel': 950,\n",
       " 'regard': 951,\n",
       " 'predominant': 952,\n",
       " 'scholary': 953,\n",
       " 'consensus': 954,\n",
       " 'allegedly': 955,\n",
       " 'despite': 956,\n",
       " 'rhetoric': 957,\n",
       " 'fascism': 958,\n",
       " 'functioned': 959,\n",
       " 'consistently': 960,\n",
       " 'rightwing': 961,\n",
       " 'force': 962,\n",
       " 'aware': 963,\n",
       " 'owning': 964,\n",
       " 'numerous': 965,\n",
       " 'book': 966,\n",
       " 'developed': 967,\n",
       " 'scholar': 968,\n",
       " 'manner': 969,\n",
       " 'bias': 970,\n",
       " 'roger': 971,\n",
       " 'griffin': 972,\n",
       " 'hamish': 973,\n",
       " 'mcdonald': 974,\n",
       " 'eatwell': 975,\n",
       " 'zeev': 976,\n",
       " 'sternhell': 977,\n",
       " 'recongise': 978,\n",
       " 'show': 979,\n",
       " 'dissenter': 980,\n",
       " 'seem': 981,\n",
       " 'absoutely': 982,\n",
       " 'leftist': 983,\n",
       " 'connection': 984,\n",
       " 'radical': 985,\n",
       " 'right': 986,\n",
       " 'system': 987,\n",
       " 'street': 988,\n",
       " 'socialist': 989,\n",
       " 'put': 990,\n",
       " 'distance': 991,\n",
       " 'movement': 992,\n",
       " 'course': 993,\n",
       " 'educated': 994,\n",
       " 'foremost': 995,\n",
       " 'expert': 996,\n",
       " 'former': 997,\n",
       " 'member': 998,\n",
       " 'communist': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '[PAD]',\n",
       " 1: 'explanation',\n",
       " 2: 'edits',\n",
       " 3: 'made',\n",
       " 4: 'username',\n",
       " 5: 'hardcore',\n",
       " 6: 'metallica',\n",
       " 7: 'fan',\n",
       " 8: 'reverted',\n",
       " 9: 'vandalism',\n",
       " 10: 'closure',\n",
       " 11: 'gas',\n",
       " 12: 'voted',\n",
       " 13: 'new',\n",
       " 14: 'york',\n",
       " 15: 'doll',\n",
       " 16: 'fac',\n",
       " 17: 'please',\n",
       " 18: 'remove',\n",
       " 19: 'template',\n",
       " 20: 'talk',\n",
       " 21: 'page',\n",
       " 22: 'since',\n",
       " 23: 'retired',\n",
       " 24: 'daww',\n",
       " 25: 'match',\n",
       " 26: 'background',\n",
       " 27: 'colour',\n",
       " 28: 'seemingly',\n",
       " 29: 'stuck',\n",
       " 30: 'thanks',\n",
       " 31: 'january',\n",
       " 32: 'utc',\n",
       " 33: 'hey',\n",
       " 34: 'man',\n",
       " 35: 'really',\n",
       " 36: 'trying',\n",
       " 37: 'edit',\n",
       " 38: 'war',\n",
       " 39: 'guy',\n",
       " 40: 'constantly',\n",
       " 41: 'removing',\n",
       " 42: 'relevant',\n",
       " 43: 'information',\n",
       " 44: 'talking',\n",
       " 45: 'instead',\n",
       " 46: 'seems',\n",
       " 47: 'care',\n",
       " 48: 'formatting',\n",
       " 49: 'actual',\n",
       " 50: 'info',\n",
       " 51: 'make',\n",
       " 52: 'real',\n",
       " 53: 'suggestion',\n",
       " 54: 'improvement',\n",
       " 55: 'wondered',\n",
       " 56: 'section',\n",
       " 57: 'statistic',\n",
       " 58: 'later',\n",
       " 59: 'subsection',\n",
       " 60: 'type',\n",
       " 61: 'accident',\n",
       " 62: 'think',\n",
       " 63: 'reference',\n",
       " 64: 'may',\n",
       " 65: 'need',\n",
       " 66: 'tidying',\n",
       " 67: 'exact',\n",
       " 68: 'format',\n",
       " 69: 'ie',\n",
       " 70: 'date',\n",
       " 71: 'etc',\n",
       " 72: 'noone',\n",
       " 73: 'else',\n",
       " 74: 'first',\n",
       " 75: 'preference',\n",
       " 76: 'style',\n",
       " 77: 'want',\n",
       " 78: 'let',\n",
       " 79: 'know',\n",
       " 80: 'appears',\n",
       " 81: 'backlog',\n",
       " 82: 'article',\n",
       " 83: 'review',\n",
       " 84: 'guess',\n",
       " 85: 'delay',\n",
       " 86: 'reviewer',\n",
       " 87: 'turn',\n",
       " 88: 'listed',\n",
       " 89: 'form',\n",
       " 90: 'eg',\n",
       " 91: 'wikipediagoodarticlenominationstransport',\n",
       " 92: 'sir',\n",
       " 93: 'hero',\n",
       " 94: 'chance',\n",
       " 95: 'remember',\n",
       " 96: 'congratulation',\n",
       " 97: 'well',\n",
       " 98: 'use',\n",
       " 99: 'tool',\n",
       " 100: 'cocksucker',\n",
       " 101: 'piss',\n",
       " 102: 'around',\n",
       " 103: 'work',\n",
       " 104: 'matt',\n",
       " 105: 'shirvington',\n",
       " 106: 'banned',\n",
       " 107: 'sorry',\n",
       " 108: 'word',\n",
       " 109: 'nonsense',\n",
       " 110: 'offensive',\n",
       " 111: 'anyway',\n",
       " 112: 'intending',\n",
       " 113: 'write',\n",
       " 114: 'anything',\n",
       " 115: 'articlewow',\n",
       " 116: 'would',\n",
       " 117: 'jump',\n",
       " 118: 'merely',\n",
       " 119: 'requesting',\n",
       " 120: 'encyclopedic',\n",
       " 121: 'one',\n",
       " 122: 'school',\n",
       " 123: 'selective',\n",
       " 124: 'breeding',\n",
       " 125: 'almost',\n",
       " 126: 'stub',\n",
       " 127: 'point',\n",
       " 128: 'animal',\n",
       " 129: 'short',\n",
       " 130: 'messy',\n",
       " 131: 'give',\n",
       " 132: 'must',\n",
       " 133: 'someone',\n",
       " 134: 'expertise',\n",
       " 135: 'eugenics',\n",
       " 136: 'alignment',\n",
       " 137: 'subject',\n",
       " 138: 'contrary',\n",
       " 139: 'dulithgow',\n",
       " 140: 'fair',\n",
       " 141: 'rationale',\n",
       " 142: 'imagewonjujpg',\n",
       " 143: 'uploading',\n",
       " 144: 'notice',\n",
       " 145: 'image',\n",
       " 146: 'specifies',\n",
       " 147: 'used',\n",
       " 148: 'wikipedia',\n",
       " 149: 'constitutes',\n",
       " 150: 'addition',\n",
       " 151: 'boilerplate',\n",
       " 152: 'also',\n",
       " 153: 'description',\n",
       " 154: 'specific',\n",
       " 155: 'using',\n",
       " 156: 'consistent',\n",
       " 157: 'go',\n",
       " 158: 'include',\n",
       " 159: 'uploaded',\n",
       " 160: 'medium',\n",
       " 161: 'consider',\n",
       " 162: 'checking',\n",
       " 163: 'specified',\n",
       " 164: 'find',\n",
       " 165: 'list',\n",
       " 166: 'edited',\n",
       " 167: 'clicking',\n",
       " 168: 'contribution',\n",
       " 169: 'link',\n",
       " 170: 'located',\n",
       " 171: 'top',\n",
       " 172: 'logged',\n",
       " 173: 'selecting',\n",
       " 174: 'dropdown',\n",
       " 175: 'box',\n",
       " 176: 'note',\n",
       " 177: 'lacking',\n",
       " 178: 'deleted',\n",
       " 179: 'week',\n",
       " 180: 'described',\n",
       " 181: 'criterion',\n",
       " 182: 'speedy',\n",
       " 183: 'deletion',\n",
       " 184: 'question',\n",
       " 185: 'ask',\n",
       " 186: 'copyright',\n",
       " 187: 'thank',\n",
       " 188: 'contribs',\n",
       " 189: 'unspecified',\n",
       " 190: 'source',\n",
       " 191: 'noticed',\n",
       " 192: 'file',\n",
       " 193: 'currently',\n",
       " 194: 'specify',\n",
       " 195: 'created',\n",
       " 196: 'content',\n",
       " 197: 'status',\n",
       " 198: 'unclear',\n",
       " 199: 'create',\n",
       " 200: 'owner',\n",
       " 201: 'obtained',\n",
       " 202: 'website',\n",
       " 203: 'taken',\n",
       " 204: 'together',\n",
       " 205: 'restatement',\n",
       " 206: 'term',\n",
       " 207: 'usually',\n",
       " 208: 'sufficient',\n",
       " 209: 'however',\n",
       " 210: 'holder',\n",
       " 211: 'different',\n",
       " 212: 'publisher',\n",
       " 213: 'acknowledged',\n",
       " 214: 'adding',\n",
       " 215: 'add',\n",
       " 216: 'proper',\n",
       " 217: 'licensing',\n",
       " 218: 'tag',\n",
       " 219: 'already',\n",
       " 220: 'createdtook',\n",
       " 221: 'picture',\n",
       " 222: 'audio',\n",
       " 223: 'video',\n",
       " 224: 'release',\n",
       " 225: 'gfdl',\n",
       " 226: 'believe',\n",
       " 227: 'meet',\n",
       " 228: 'wikipediafair',\n",
       " 229: 'wikipediaimage',\n",
       " 230: 'tagsfair',\n",
       " 231: 'see',\n",
       " 232: 'full',\n",
       " 233: 'tagged',\n",
       " 234: 'following',\n",
       " 235: 'unsourced',\n",
       " 236: 'untagged',\n",
       " 237: 'copyrighted',\n",
       " 238: 'nonfree',\n",
       " 239: 'license',\n",
       " 240: 'per',\n",
       " 241: 'hour',\n",
       " 242: 'bbq',\n",
       " 243: 'u',\n",
       " 244: 'discus',\n",
       " 245: 'itmaybe',\n",
       " 246: 'phone',\n",
       " 247: 'exclusive',\n",
       " 248: 'group',\n",
       " 249: 'wp',\n",
       " 250: 'talibanswho',\n",
       " 251: 'good',\n",
       " 252: 'destroying',\n",
       " 253: 'selfappointed',\n",
       " 254: 'purist',\n",
       " 255: 'gang',\n",
       " 256: 'asks',\n",
       " 257: 'antisocial',\n",
       " 258: 'destructive',\n",
       " 259: 'noncontribution',\n",
       " 260: 'sityush',\n",
       " 261: 'clean',\n",
       " 262: 'behavior',\n",
       " 263: 'issue',\n",
       " 264: 'nonsensical',\n",
       " 265: 'warning',\n",
       " 266: 'start',\n",
       " 267: 'throwing',\n",
       " 268: 'accusation',\n",
       " 269: 'itselfmaking',\n",
       " 270: 'ad',\n",
       " 271: 'hominem',\n",
       " 272: 'attack',\n",
       " 273: 'going',\n",
       " 274: 'strengthen',\n",
       " 275: 'argument',\n",
       " 276: 'look',\n",
       " 277: 'like',\n",
       " 278: 'abusing',\n",
       " 279: 'power',\n",
       " 280: 'admin',\n",
       " 281: 'relevantthis',\n",
       " 282: 'probably',\n",
       " 283: 'single',\n",
       " 284: 'talked',\n",
       " 285: 'event',\n",
       " 286: 'int',\n",
       " 287: 'news',\n",
       " 288: 'late',\n",
       " 289: 'absence',\n",
       " 290: 'notable',\n",
       " 291: 'living',\n",
       " 292: 'expresident',\n",
       " 293: 'attend',\n",
       " 294: 'certainly',\n",
       " 295: 'dedicating',\n",
       " 296: 'aircracft',\n",
       " 297: 'carrier',\n",
       " 298: 'intend',\n",
       " 299: 'revert',\n",
       " 300: 'hope',\n",
       " 301: 'attracting',\n",
       " 302: 'attention',\n",
       " 303: 'willing',\n",
       " 304: 'throw',\n",
       " 305: 'quite',\n",
       " 306: 'liberally',\n",
       " 307: 'perhaps',\n",
       " 308: 'achieve',\n",
       " 309: 'level',\n",
       " 310: 'civility',\n",
       " 311: 'rational',\n",
       " 312: 'discussion',\n",
       " 313: 'topic',\n",
       " 314: 'resolve',\n",
       " 315: 'matter',\n",
       " 316: 'peacefully',\n",
       " 317: 'oh',\n",
       " 318: 'girl',\n",
       " 319: 'started',\n",
       " 320: 'nose',\n",
       " 321: 'belong',\n",
       " 322: 'yvesnimmo',\n",
       " 323: 'said',\n",
       " 324: 'situation',\n",
       " 325: 'settled',\n",
       " 326: 'apologized',\n",
       " 327: 'juelz',\n",
       " 328: 'santanas',\n",
       " 329: 'age',\n",
       " 330: 'santana',\n",
       " 331: 'year',\n",
       " 332: 'old',\n",
       " 333: 'came',\n",
       " 334: 'february',\n",
       " 335: 'th',\n",
       " 336: 'making',\n",
       " 337: 'song',\n",
       " 338: 'diplomat',\n",
       " 339: 'third',\n",
       " 340: 'neff',\n",
       " 341: 'signed',\n",
       " 342: 'cam',\n",
       " 343: 'label',\n",
       " 344: 'roc',\n",
       " 345: 'fella',\n",
       " 346: 'coming',\n",
       " 347: 'town',\n",
       " 348: 'yes',\n",
       " 349: 'born',\n",
       " 350: 'could',\n",
       " 351: 'older',\n",
       " 352: 'lloyd',\n",
       " 353: 'bank',\n",
       " 354: 'birthday',\n",
       " 355: 'passed',\n",
       " 356: 'homie',\n",
       " 357: 'death',\n",
       " 358: 'god',\n",
       " 359: 'forbid',\n",
       " 360: 'thinking',\n",
       " 361: 'equal',\n",
       " 362: 'caculator',\n",
       " 363: 'stop',\n",
       " 364: 'changing',\n",
       " 365: 'birth',\n",
       " 366: 'bye',\n",
       " 367: 'come',\n",
       " 368: 'comming',\n",
       " 369: 'back',\n",
       " 370: 'tosser',\n",
       " 371: 'redirect',\n",
       " 372: 'talkvoydan',\n",
       " 373: 'pop',\n",
       " 374: 'georgiev',\n",
       " 375: 'chernodrinski',\n",
       " 376: 'mitsurugi',\n",
       " 377: 'sense',\n",
       " 378: 'argue',\n",
       " 379: 'hindi',\n",
       " 380: 'ryo',\n",
       " 381: 'sakazakis',\n",
       " 382: 'mean',\n",
       " 383: 'bother',\n",
       " 384: 'writing',\n",
       " 385: 'something',\n",
       " 386: 'regarding',\n",
       " 387: 'posted',\n",
       " 388: 'acctually',\n",
       " 389: 'even',\n",
       " 390: 'better',\n",
       " 391: 'take',\n",
       " 392: 'closer',\n",
       " 393: 'premature',\n",
       " 394: 'wrestling',\n",
       " 395: 'catagory',\n",
       " 396: 'men',\n",
       " 397: 'surely',\n",
       " 398: 'besides',\n",
       " 399: 'delting',\n",
       " 400: 'recent',\n",
       " 401: 'read',\n",
       " 402: 'wpfilmplot',\n",
       " 403: 'editing',\n",
       " 404: 'film',\n",
       " 405: 'simply',\n",
       " 406: 'entirely',\n",
       " 407: 'many',\n",
       " 408: 'unnecessary',\n",
       " 409: 'detail',\n",
       " 410: 'bad',\n",
       " 411: 'damage',\n",
       " 412: 'yeah',\n",
       " 413: 'studying',\n",
       " 414: 'nowdeepu',\n",
       " 415: 'snowflake',\n",
       " 416: 'always',\n",
       " 417: 'symmetrical',\n",
       " 418: 'geometry',\n",
       " 419: 'stated',\n",
       " 420: 'six',\n",
       " 421: 'symmetric',\n",
       " 422: 'arm',\n",
       " 423: 'assertion',\n",
       " 424: 'true',\n",
       " 425: 'according',\n",
       " 426: 'kenneth',\n",
       " 427: 'libbrecht',\n",
       " 428: 'rather',\n",
       " 429: 'unattractive',\n",
       " 430: 'irregular',\n",
       " 431: 'crystal',\n",
       " 432: 'far',\n",
       " 433: 'common',\n",
       " 434: 'variety',\n",
       " 435: 'site',\n",
       " 436: 'get',\n",
       " 437: 'fact',\n",
       " 438: 'still',\n",
       " 439: 'decent',\n",
       " 440: 'number',\n",
       " 441: 'falsity',\n",
       " 442: 'forgive',\n",
       " 443: 'signpost',\n",
       " 444: 'september',\n",
       " 445: 'singlepage',\n",
       " 446: 'unsubscribe',\n",
       " 447: 'reconsidering',\n",
       " 448: 'st',\n",
       " 449: 'paragraph',\n",
       " 450: 'understand',\n",
       " 451: 'reason',\n",
       " 452: 'sure',\n",
       " 453: 'data',\n",
       " 454: 'necessarily',\n",
       " 455: 'wrong',\n",
       " 456: 'persuaded',\n",
       " 457: 'strategy',\n",
       " 458: 'introducing',\n",
       " 459: 'academic',\n",
       " 460: 'honor',\n",
       " 461: 'unhelpful',\n",
       " 462: 'approach',\n",
       " 463: 'sitting',\n",
       " 464: 'justice',\n",
       " 465: 'similarly',\n",
       " 466: 'enhanced',\n",
       " 467: 'change',\n",
       " 468: 'support',\n",
       " 469: 'view',\n",
       " 470: 'invite',\n",
       " 471: 'anyone',\n",
       " 472: 'revisit',\n",
       " 473: 'written',\n",
       " 474: 'pair',\n",
       " 475: 'jurist',\n",
       " 476: 'benjamin',\n",
       " 477: 'cardozo',\n",
       " 478: 'learned',\n",
       " 479: 'hand',\n",
       " 480: 'b',\n",
       " 481: 'john',\n",
       " 482: 'marshall',\n",
       " 483: 'harlan',\n",
       " 484: 'ii',\n",
       " 485: 'becomes',\n",
       " 486: 'current',\n",
       " 487: 'version',\n",
       " 488: 'either',\n",
       " 489: 'improved',\n",
       " 490: 'credential',\n",
       " 491: 'introductory',\n",
       " 492: 'help',\n",
       " 493: 'repeat',\n",
       " 494: 'wry',\n",
       " 495: 'kathleen',\n",
       " 496: 'sullivan',\n",
       " 497: 'stanford',\n",
       " 498: 'law',\n",
       " 499: 'suggests',\n",
       " 500: 'harvard',\n",
       " 501: 'faculty',\n",
       " 502: 'wonder',\n",
       " 503: 'antonin',\n",
       " 504: 'scalia',\n",
       " 505: 'avoided',\n",
       " 506: 'learning',\n",
       " 507: 'others',\n",
       " 508: 'managed',\n",
       " 509: 'grasp',\n",
       " 510: 'process',\n",
       " 511: 'judging',\n",
       " 512: 'anecdote',\n",
       " 513: 'gently',\n",
       " 514: 'illustrates',\n",
       " 515: 'le',\n",
       " 516: 'humorous',\n",
       " 517: 'stronger',\n",
       " 518: 'clarence',\n",
       " 519: 'thomas',\n",
       " 520: 'mention',\n",
       " 521: 'wanting',\n",
       " 522: 'return',\n",
       " 523: 'degree',\n",
       " 524: 'yale',\n",
       " 525: 'minimum',\n",
       " 526: 'questioning',\n",
       " 527: 'deserves',\n",
       " 528: 'reconsidered',\n",
       " 529: 'radial',\n",
       " 530: 'symmetry',\n",
       " 531: 'several',\n",
       " 532: 'extinct',\n",
       " 533: 'lineage',\n",
       " 534: 'included',\n",
       " 535: 'echinodermata',\n",
       " 536: 'bilateral',\n",
       " 537: 'homostelea',\n",
       " 538: 'asymmetrical',\n",
       " 539: 'cothurnocystis',\n",
       " 540: 'stylophora',\n",
       " 541: 'apologize',\n",
       " 542: 'reconciling',\n",
       " 543: 'knowledge',\n",
       " 544: 'done',\n",
       " 545: 'history',\n",
       " 546: 'study',\n",
       " 547: 'archaeology',\n",
       " 548: 'scan',\n",
       " 549: 'email',\n",
       " 550: 'translate',\n",
       " 551: 'mother',\n",
       " 552: 'child',\n",
       " 553: 'case',\n",
       " 554: 'michael',\n",
       " 555: 'jackson',\n",
       " 556: 'studied',\n",
       " 557: 'motif',\n",
       " 558: 'reasoning',\n",
       " 559: 'judged',\n",
       " 560: 'upon',\n",
       " 561: 'character',\n",
       " 562: 'harshly',\n",
       " 563: 'wacko',\n",
       " 564: 'jacko',\n",
       " 565: 'tell',\n",
       " 566: 'ignore',\n",
       " 567: 'incriminate',\n",
       " 568: 'continue',\n",
       " 569: 'refuting',\n",
       " 570: 'bullshit',\n",
       " 571: 'jayjg',\n",
       " 572: 'keep',\n",
       " 573: 'jun',\n",
       " 574: 'ok',\n",
       " 575: 'bit',\n",
       " 576: 'example',\n",
       " 577: 'base',\n",
       " 578: 'duck',\n",
       " 579: 'barnstar',\n",
       " 580: 'life',\n",
       " 581: 'star',\n",
       " 582: 'post',\n",
       " 583: 'block',\n",
       " 584: 'expires',\n",
       " 585: 'funny',\n",
       " 586: 'thing',\n",
       " 587: 'uncivil',\n",
       " 588: 'heading',\n",
       " 589: 'fight',\n",
       " 590: 'freedom',\n",
       " 591: 'contain',\n",
       " 592: 'praise',\n",
       " 593: 'looked',\n",
       " 594: 'month',\n",
       " 595: 'ago',\n",
       " 596: 'much',\n",
       " 597: 'able',\n",
       " 598: 'quickly',\n",
       " 599: 'text',\n",
       " 600: 'hard',\n",
       " 601: 'drive',\n",
       " 602: 'meaning',\n",
       " 603: 'updating',\n",
       " 604: 'sound',\n",
       " 605: 'time',\n",
       " 606: 'generating',\n",
       " 607: 'interest',\n",
       " 608: 'spent',\n",
       " 609: 'four',\n",
       " 610: 'drum',\n",
       " 611: 'freely',\n",
       " 612: 'licensed',\n",
       " 613: 'length',\n",
       " 614: 'classical',\n",
       " 615: 'music',\n",
       " 616: 'unfortunately',\n",
       " 617: 'attempt',\n",
       " 618: 'failed',\n",
       " 619: 'effectively',\n",
       " 620: 'wikiproject',\n",
       " 621: 'interested',\n",
       " 622: 'wikipediatalkwikiprojectclassicalmusicarchive',\n",
       " 623: 'needhelp',\n",
       " 624: 'wikipediatalkwikiprojectmusicarchive',\n",
       " 625: 'icouldusesomehelpwikipediatalkwikiprojectmusicarchive',\n",
       " 626: 'raulbot',\n",
       " 627: 'candthemusiclist',\n",
       " 628: 'given',\n",
       " 629: 'featured',\n",
       " 630: 'digg',\n",
       " 631: 'got',\n",
       " 632: 'diggs',\n",
       " 633: 'impressive',\n",
       " 634: 'subpages',\n",
       " 635: 'rfa',\n",
       " 636: 'noseptembers',\n",
       " 637: 'difference',\n",
       " 638: 'elc',\n",
       " 639: 'surprised',\n",
       " 640: 'left',\n",
       " 641: 'tc',\n",
       " 642: 'straw',\n",
       " 643: 'never',\n",
       " 644: 'claimed',\n",
       " 645: 'odonohue',\n",
       " 646: 'position',\n",
       " 647: 'practitioner',\n",
       " 648: 'researcher',\n",
       " 649: 'field',\n",
       " 650: 'ignored',\n",
       " 651: 'dsm',\n",
       " 652: 'exactly',\n",
       " 653: 'quote',\n",
       " 654: 'say',\n",
       " 655: 'agrees',\n",
       " 656: 'combating',\n",
       " 657: 'notion',\n",
       " 658: 'absurd',\n",
       " 659: 'part',\n",
       " 660: 'claim',\n",
       " 661: 'pedophilia',\n",
       " 662: 'sexual',\n",
       " 663: 'orientation',\n",
       " 664: 'hold',\n",
       " 665: 'unfair',\n",
       " 666: 'call',\n",
       " 667: 'disorder',\n",
       " 668: 'divided',\n",
       " 669: 'end',\n",
       " 670: 'day',\n",
       " 671: 'value',\n",
       " 672: 'judgment',\n",
       " 673: 'cantor',\n",
       " 674: 'pointed',\n",
       " 675: 'earlier',\n",
       " 676: 'thread',\n",
       " 677: 'scientific',\n",
       " 678: 'judgement',\n",
       " 679: 'choose',\n",
       " 680: 'clearly',\n",
       " 681: 'pretend',\n",
       " 682: 'basis',\n",
       " 683: 'mainland',\n",
       " 684: 'asia',\n",
       " 685: 'includes',\n",
       " 686: 'lower',\n",
       " 687: 'basin',\n",
       " 688: 'china',\n",
       " 689: 'yangtze',\n",
       " 690: 'river',\n",
       " 691: 'korea',\n",
       " 692: 'fine',\n",
       " 693: 'found',\n",
       " 694: 'citation',\n",
       " 695: 'comprehensive',\n",
       " 696: 'dna',\n",
       " 697: 'hammer',\n",
       " 698: 'generarizations',\n",
       " 699: 'speculation',\n",
       " 700: 'yayoi',\n",
       " 701: 'culture',\n",
       " 702: 'brought',\n",
       " 703: 'japan',\n",
       " 704: 'migrant',\n",
       " 705: 'trace',\n",
       " 706: 'root',\n",
       " 707: 'southeast',\n",
       " 708: 'asiasouth',\n",
       " 709: 'describes',\n",
       " 710: 'migration',\n",
       " 711: 'based',\n",
       " 712: 'osry',\n",
       " 713: 'gene',\n",
       " 714: 'close',\n",
       " 715: 'haplogroups',\n",
       " 716: 'om',\n",
       " 717: 'reiterates',\n",
       " 718: 'entire',\n",
       " 719: 'haplogroup',\n",
       " 720: 'proposed',\n",
       " 721: 'asian',\n",
       " 722: 'origin',\n",
       " 723: 'definition',\n",
       " 724: 'southern',\n",
       " 725: 'hypothesizes',\n",
       " 726: 'dispersal',\n",
       " 727: 'neolithic',\n",
       " 728: 'farmer',\n",
       " 729: 'eventually',\n",
       " 730: 'concluding',\n",
       " 731: 'state',\n",
       " 732: 'propose',\n",
       " 733: 'chromosome',\n",
       " 734: 'descend',\n",
       " 735: 'prehistoric',\n",
       " 736: 'southeastern',\n",
       " 737: 'agriculture',\n",
       " 738: 'region',\n",
       " 739: 'global',\n",
       " 740: 'sample',\n",
       " 741: 'consisted',\n",
       " 742: 'male',\n",
       " 743: 'population',\n",
       " 744: 'including',\n",
       " 745: 'sampled',\n",
       " 746: 'across',\n",
       " 747: 'japanese',\n",
       " 748: 'archipelago',\n",
       " 749: 'pretty',\n",
       " 750: 'everyone',\n",
       " 751: 'warren',\n",
       " 752: 'countysurrounding',\n",
       " 753: 'glen',\n",
       " 754: 'fall',\n",
       " 755: 'hospital',\n",
       " 756: 'qualifies',\n",
       " 757: 'native',\n",
       " 758: 'rachel',\n",
       " 759: 'ray',\n",
       " 760: 'actually',\n",
       " 761: 'lake',\n",
       " 762: 'luzerne',\n",
       " 763: 'preceding',\n",
       " 764: 'unsigned',\n",
       " 765: 'comment',\n",
       " 766: 'added',\n",
       " 767: 'august',\n",
       " 768: 'hi',\n",
       " 769: 'explicit',\n",
       " 770: 'fenian',\n",
       " 771: 'editwarring',\n",
       " 772: 'giant',\n",
       " 773: 'causeway',\n",
       " 774: 'terrorism',\n",
       " 775: 'notability',\n",
       " 776: 'rurika',\n",
       " 777: 'kasuga',\n",
       " 778: 'placed',\n",
       " 779: 'speedily',\n",
       " 780: 'person',\n",
       " 781: 'people',\n",
       " 782: 'band',\n",
       " 783: 'club',\n",
       " 784: 'company',\n",
       " 785: 'web',\n",
       " 786: 'indicate',\n",
       " 787: 'assert',\n",
       " 788: 'guideline',\n",
       " 789: 'generally',\n",
       " 790: 'accepted',\n",
       " 791: 'contest',\n",
       " 792: 'tagging',\n",
       " 793: 'existing',\n",
       " 794: 'db',\n",
       " 795: 'leave',\n",
       " 796: 'explaining',\n",
       " 797: 'hesitate',\n",
       " 798: 'confirm',\n",
       " 799: 'check',\n",
       " 800: 'biography',\n",
       " 801: 'feel',\n",
       " 802: 'free',\n",
       " 803: 'lead',\n",
       " 804: 'briefly',\n",
       " 805: 'summarize',\n",
       " 806: 'armenia',\n",
       " 807: 'necessary',\n",
       " 808: 'sentence',\n",
       " 809: 'redundant',\n",
       " 810: 'welcome',\n",
       " 811: 'tfd',\n",
       " 812: 'eced',\n",
       " 813: 'responded',\n",
       " 814: 'without',\n",
       " 815: 'seeing',\n",
       " 816: 'response',\n",
       " 817: 'saw',\n",
       " 818: 'mine',\n",
       " 819: 'tcwpchicagowpfour',\n",
       " 820: 'gay',\n",
       " 821: 'antisemmitian',\n",
       " 822: 'archangel',\n",
       " 823: 'white',\n",
       " 824: 'tiger',\n",
       " 825: 'meow',\n",
       " 826: 'greetingshhh',\n",
       " 827: 'uh',\n",
       " 828: 'two',\n",
       " 829: 'way',\n",
       " 830: 'erased',\n",
       " 831: 'ww',\n",
       " 832: 'holocaust',\n",
       " 833: 'brutally',\n",
       " 834: 'slaying',\n",
       " 835: 'jew',\n",
       " 836: 'gaysgypsysslavsanyone',\n",
       " 837: 'antisemitian',\n",
       " 838: 'shave',\n",
       " 839: 'head',\n",
       " 840: 'bald',\n",
       " 841: 'skinhead',\n",
       " 842: 'meeting',\n",
       " 843: 'doubt',\n",
       " 844: 'bible',\n",
       " 845: 'homosexuality',\n",
       " 846: 'deadly',\n",
       " 847: 'sin',\n",
       " 848: 'pentagram',\n",
       " 849: 'tatoo',\n",
       " 850: 'forehead',\n",
       " 851: 'satanistic',\n",
       " 852: 'mass',\n",
       " 853: 'pal',\n",
       " 854: 'last',\n",
       " 855: 'fucking',\n",
       " 856: 'appreciate',\n",
       " 857: 'nazi',\n",
       " 858: 'shwain',\n",
       " 859: 'wish',\n",
       " 860: 'anymore',\n",
       " 861: 'beware',\n",
       " 862: 'dark',\n",
       " 863: 'side',\n",
       " 864: 'fuck',\n",
       " 865: 'filthy',\n",
       " 866: 'as',\n",
       " 867: 'dry',\n",
       " 868: 'screwed',\n",
       " 869: 'dominance',\n",
       " 870: 'bow',\n",
       " 871: 'almighty',\n",
       " 872: 'administrator',\n",
       " 873: 'play',\n",
       " 874: 'outsidewith',\n",
       " 875: 'mom',\n",
       " 876: 'lisak',\n",
       " 877: 'criticism',\n",
       " 878: 'present',\n",
       " 879: 'conforms',\n",
       " 880: 'npv',\n",
       " 881: 'rule',\n",
       " 882: 'neutral',\n",
       " 883: 'begin',\n",
       " 884: 'offer',\n",
       " 885: 'polygraph',\n",
       " 886: 'concerned',\n",
       " 887: 'result',\n",
       " 888: 'shock',\n",
       " 889: 'complainant',\n",
       " 890: 'lie',\n",
       " 891: 'uncovered',\n",
       " 892: 'recantation',\n",
       " 893: 'perfectly',\n",
       " 894: 'valid',\n",
       " 895: 'telling',\n",
       " 896: 'truth',\n",
       " 897: 'machine',\n",
       " 898: 'investigator',\n",
       " 899: 'kanins',\n",
       " 900: 'research',\n",
       " 901: 'followup',\n",
       " 902: 'recanted',\n",
       " 903: 'story',\n",
       " 904: 'possible',\n",
       " 905: 'verify',\n",
       " 906: 'false',\n",
       " 907: 'matched',\n",
       " 908: 'accused',\n",
       " 909: 'happened',\n",
       " 910: 'arguing',\n",
       " 911: 'respected',\n",
       " 912: 'phd',\n",
       " 913: 'baseless',\n",
       " 914: 'kanin',\n",
       " 915: 'agree',\n",
       " 916: 'though',\n",
       " 917: 'ammended',\n",
       " 918: 'appropriate',\n",
       " 919: 'notabilitysignificance',\n",
       " 920: 'lazy',\n",
       " 921: 'stalking',\n",
       " 922: 'absolute',\n",
       " 923: 'rubbish',\n",
       " 924: 'serf',\n",
       " 925: 'aggravate',\n",
       " 926: 'assumed',\n",
       " 927: 'faith',\n",
       " 928: 'intention',\n",
       " 929: 'suggested',\n",
       " 930: 'seen',\n",
       " 931: 'suggest',\n",
       " 932: 'might',\n",
       " 933: 'ulterior',\n",
       " 934: 'motive',\n",
       " 935: 'massadding',\n",
       " 936: 'ever',\n",
       " 937: 'administrative',\n",
       " 938: 'mentioned',\n",
       " 939: 'role',\n",
       " 940: 'party',\n",
       " 941: 'disagreement',\n",
       " 942: 'rate',\n",
       " 943: 'conflict',\n",
       " 944: 'thus',\n",
       " 945: 'extend',\n",
       " 946: 'toward',\n",
       " 947: 'spurious',\n",
       " 948: 'unfounded',\n",
       " 949: 'chatspy',\n",
       " 950: 'jmabel',\n",
       " 951: 'regard',\n",
       " 952: 'predominant',\n",
       " 953: 'scholary',\n",
       " 954: 'consensus',\n",
       " 955: 'allegedly',\n",
       " 956: 'despite',\n",
       " 957: 'rhetoric',\n",
       " 958: 'fascism',\n",
       " 959: 'functioned',\n",
       " 960: 'consistently',\n",
       " 961: 'rightwing',\n",
       " 962: 'force',\n",
       " 963: 'aware',\n",
       " 964: 'owning',\n",
       " 965: 'numerous',\n",
       " 966: 'book',\n",
       " 967: 'developed',\n",
       " 968: 'scholar',\n",
       " 969: 'manner',\n",
       " 970: 'bias',\n",
       " 971: 'roger',\n",
       " 972: 'griffin',\n",
       " 973: 'hamish',\n",
       " 974: 'mcdonald',\n",
       " 975: 'eatwell',\n",
       " 976: 'zeev',\n",
       " 977: 'sternhell',\n",
       " 978: 'recongise',\n",
       " 979: 'show',\n",
       " 980: 'dissenter',\n",
       " 981: 'seem',\n",
       " 982: 'absoutely',\n",
       " 983: 'leftist',\n",
       " 984: 'connection',\n",
       " 985: 'radical',\n",
       " 986: 'right',\n",
       " 987: 'system',\n",
       " 988: 'street',\n",
       " 989: 'socialist',\n",
       " 990: 'put',\n",
       " 991: 'distance',\n",
       " 992: 'movement',\n",
       " 993: 'course',\n",
       " 994: 'educated',\n",
       " 995: 'foremost',\n",
       " 996: 'expert',\n",
       " 997: 'former',\n",
       " 998: 'member',\n",
       " 999: 'communist',\n",
       " ...}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ettentio',\n",
       " 'agresion',\n",
       " 'gambon',\n",
       " 'napeolonic',\n",
       " 'kokobo',\n",
       " 'kyd',\n",
       " 'seriouslyfolks',\n",
       " 'userakraj',\n",
       " 'pleny',\n",
       " 'grounded',\n",
       " 'raychaudhuri',\n",
       " 'welladvised',\n",
       " 'wulfsbergs',\n",
       " 'reassessmenthalfway',\n",
       " 'nowcoldplay',\n",
       " 'dictorial',\n",
       " 'tandy',\n",
       " 'uniti',\n",
       " 'skyisblue',\n",
       " 'sarsa',\n",
       " 'northholland',\n",
       " 'kitaky',\n",
       " 'bedevilment',\n",
       " 'aquinas',\n",
       " 'apprenticefan',\n",
       " 'asslicking',\n",
       " 'middleschoolers',\n",
       " 'knowledgesharing',\n",
       " 'familiarising',\n",
       " 'lithgows',\n",
       " 'tortuous',\n",
       " 'inquiriesstyle',\n",
       " 'optically',\n",
       " 'conven',\n",
       " 'imageheroess',\n",
       " 'cooridinates',\n",
       " 'fffaef',\n",
       " 'talkwatts',\n",
       " 'theni',\n",
       " 'subalpine',\n",
       " 'haiducs',\n",
       " 'thisplease',\n",
       " 'elitexc',\n",
       " 'deletedreverted',\n",
       " 'generous',\n",
       " 'selfjustifications',\n",
       " 'permeates',\n",
       " 'fattened',\n",
       " 'rius',\n",
       " 'manics',\n",
       " 'hisin',\n",
       " 'vintner',\n",
       " 'probable',\n",
       " 'stsot',\n",
       " 'conflictrussian',\n",
       " 'respectthat',\n",
       " 'mihail',\n",
       " 'userwikifunusa',\n",
       " 'beowulf',\n",
       " 'mitochrondrial',\n",
       " 'wfrwikipediastatistiques',\n",
       " 'heighho',\n",
       " 'cockonce',\n",
       " 'rcs',\n",
       " 'interessterd',\n",
       " 'copilot',\n",
       " 'selfpublicity',\n",
       " 'resize',\n",
       " 'gensammelten',\n",
       " 'elitest',\n",
       " 'homogenous',\n",
       " 'beahiviour',\n",
       " 'flog',\n",
       " 'exicuted',\n",
       " 'schwitter',\n",
       " 'thehenry',\n",
       " 'linschoten',\n",
       " 'wwould',\n",
       " 'steelman',\n",
       " 'foundators',\n",
       " 'terminados',\n",
       " 'decentthat',\n",
       " 'wainwright',\n",
       " 'userkaraman',\n",
       " 'coffeeremember',\n",
       " 'eucharist',\n",
       " 'nonhistorians',\n",
       " 'perls',\n",
       " 'radni',\n",
       " 'str',\n",
       " 'jigsaw',\n",
       " 'casulties',\n",
       " 'dbnot',\n",
       " 'historicalpoliticalsocial',\n",
       " 'zeimusu',\n",
       " 'unprofessional',\n",
       " 'brandicactus',\n",
       " 'zoroastrian',\n",
       " 'blart',\n",
       " 'categoryintelligent',\n",
       " 'prachi',\n",
       " 'squareintegrable',\n",
       " 'marraige',\n",
       " 'wpeditconsensus',\n",
       " 'lashed',\n",
       " 'prognathous',\n",
       " 'papersperiod',\n",
       " 'aria',\n",
       " 'savagerumbaugh',\n",
       " 'varduli',\n",
       " 'mythmisconception',\n",
       " 'anthologyand',\n",
       " 'auspient',\n",
       " 'feudalist',\n",
       " 'interiots',\n",
       " 'deletiontiffany',\n",
       " 'trebizond',\n",
       " 'vanishenjoy',\n",
       " 'ballclubs',\n",
       " 'quever',\n",
       " 'reconition',\n",
       " 'uhthe',\n",
       " 'cylonorg',\n",
       " 'shatter',\n",
       " 'consensusacademic',\n",
       " 'crinoline',\n",
       " 'examplesmetaphors',\n",
       " 'apprehend',\n",
       " 'mentionedwhoops',\n",
       " 'godforsaken',\n",
       " 'heinz',\n",
       " 'systemhe',\n",
       " 'handmaiden',\n",
       " 'dalma',\n",
       " 'steinsaltz',\n",
       " 'hatefilled',\n",
       " 'mandruss',\n",
       " 'suit',\n",
       " 'romanised',\n",
       " 'ksel',\n",
       " 'jafek',\n",
       " 'opengoodbadsuccessfailure',\n",
       " 'budddddyyyyy',\n",
       " 'declenisons',\n",
       " 'lakefront',\n",
       " 'wpalbumcaps',\n",
       " 'yarea',\n",
       " 'sowl',\n",
       " 'seefuchs',\n",
       " 'truejusticeorg',\n",
       " 'lowtarget',\n",
       " 'bahir',\n",
       " 'uarelated',\n",
       " 'wpanproposed',\n",
       " 'debatewithout',\n",
       " 'youin',\n",
       " 'cutter',\n",
       " 'honley',\n",
       " 'hemery',\n",
       " 'entailment',\n",
       " 'steping',\n",
       " 'userjaknouse',\n",
       " 'bused',\n",
       " 'gumby',\n",
       " 'useralamanth',\n",
       " 'dictatorshiptotalitarian',\n",
       " 'editheadz',\n",
       " 'amys',\n",
       " 'hanumakonda',\n",
       " 'kolosserfrief',\n",
       " 'shizuyo',\n",
       " 'imagecolombianfestivalukjpg',\n",
       " 'wpelac',\n",
       " 'listin',\n",
       " 'proquest',\n",
       " 'frumpy',\n",
       " 'erschallet',\n",
       " 'expliciton',\n",
       " 'unconsciously',\n",
       " 'bharatas',\n",
       " 'filament',\n",
       " 'monkys',\n",
       " 'modular',\n",
       " 'orienting',\n",
       " 'karroubi',\n",
       " 'albeit',\n",
       " 'heartbeat',\n",
       " 'commonscategoryeditathonbritishlibrary',\n",
       " 'jezeera',\n",
       " 'satellite',\n",
       " 'forexmagnates',\n",
       " 'macauley',\n",
       " 'coombs',\n",
       " 'ikalpe',\n",
       " 'besze',\n",
       " 'hardyzteam',\n",
       " 'typist',\n",
       " 'regard',\n",
       " 'hunterkiller',\n",
       " 'googlethesearchengines',\n",
       " 'megacelebrity',\n",
       " 'talksrikarkashyaptalk',\n",
       " 'uitto',\n",
       " 'hsould',\n",
       " 'emmae',\n",
       " 'sydsvenskan',\n",
       " 'salauds',\n",
       " 'rosset',\n",
       " 'minimal',\n",
       " 'trw',\n",
       " 'sprotectsmallyes',\n",
       " 'dopo',\n",
       " 'showsmoviesfashions',\n",
       " 'buddhism',\n",
       " 'countryside',\n",
       " 'councy',\n",
       " 'shiitthead',\n",
       " 'emeissions',\n",
       " 'cattle',\n",
       " 'laurahale',\n",
       " 'timc',\n",
       " 'feagans',\n",
       " 'unearthly',\n",
       " 'tsangpa',\n",
       " 'edwards',\n",
       " 'batistuta',\n",
       " 'odontoclastfurther',\n",
       " 'gripen',\n",
       " 'hapened',\n",
       " 'bifurcatory',\n",
       " 'kowey',\n",
       " 'nogod',\n",
       " 'dualism',\n",
       " 'postcold',\n",
       " 'moronics',\n",
       " 'panian',\n",
       " 'ondine',\n",
       " 'esay',\n",
       " 'ylimc',\n",
       " 'snowball',\n",
       " 'flipflop',\n",
       " 'nexis',\n",
       " 'tensai',\n",
       " 'finalfurther',\n",
       " 'amends',\n",
       " 'inscrutable',\n",
       " 'nnot',\n",
       " 'youngblood',\n",
       " 'hyperbolic',\n",
       " 'pogrom',\n",
       " 'familycult',\n",
       " 'transfixing',\n",
       " 'ubon',\n",
       " 'lumped',\n",
       " 'piasecki',\n",
       " 'shimshem',\n",
       " 'webtraffic',\n",
       " 'nairs',\n",
       " 'titlemediaexampleoggmediaexampleoggmediaexampleoggmediaexampleoggmediaexampleoggmediaexampleoggmediaexampleoggmediaexampleogginsert',\n",
       " 'selfcentered',\n",
       " 'cormie',\n",
       " 'bverkininstituteforlowtemperaturephysicsandengineering',\n",
       " 'filehugo',\n",
       " 'fage',\n",
       " 'wolterbot',\n",
       " 'relevantly',\n",
       " 'paramaters',\n",
       " 'outgoing',\n",
       " 'arsefacey',\n",
       " 'achakzai',\n",
       " 'friendprofessor',\n",
       " 'smelllllllly',\n",
       " 'homun',\n",
       " 'britannic',\n",
       " 'rhiannonnew',\n",
       " 'jasenm',\n",
       " 'cooperationhas',\n",
       " 'enclicitc',\n",
       " 'majorityuse',\n",
       " 'dws',\n",
       " 'athority',\n",
       " 'blocklogs',\n",
       " 'fenciing',\n",
       " 'yehaw',\n",
       " 'callup',\n",
       " 'muktanandas',\n",
       " 'speedier',\n",
       " 'bomber',\n",
       " 'alina',\n",
       " 'glamorising',\n",
       " 'pocketpc',\n",
       " 'novemberdecember',\n",
       " 'uopu',\n",
       " 'experincieng',\n",
       " 'hedddajjjjjjggghhhhhhh',\n",
       " 'bloons',\n",
       " 'constitutionalist',\n",
       " 'pous',\n",
       " 'dbiels',\n",
       " 'adequate',\n",
       " 'wikipediaunsuccessful',\n",
       " 'repr',\n",
       " 'succinctly',\n",
       " 'nambudiris',\n",
       " 'warthog',\n",
       " 'blockingfighting',\n",
       " 'whatsofkingever',\n",
       " 'extenuating',\n",
       " 'silass',\n",
       " 'infamously',\n",
       " 'blinde',\n",
       " 'separat',\n",
       " 'assyrianpopular',\n",
       " 'greetingshhh',\n",
       " 'waaaah',\n",
       " 'derived',\n",
       " 'jikan',\n",
       " 'nolife',\n",
       " 'cripple',\n",
       " 'singly',\n",
       " 'szuladzi',\n",
       " 'damiens',\n",
       " 'detected',\n",
       " 'pwclones',\n",
       " 'reqired',\n",
       " 'timelimit',\n",
       " 'unquestioning',\n",
       " 'cinforming',\n",
       " 'falangism',\n",
       " 'slangish',\n",
       " 'mcdork',\n",
       " 'geneflow',\n",
       " 'nouning',\n",
       " 'notmagic',\n",
       " 'violoation',\n",
       " 'uncovered',\n",
       " 'borne',\n",
       " 'jessus',\n",
       " 'correlated',\n",
       " 'valuepowerusefullness',\n",
       " 'millenarian',\n",
       " 'malksoo',\n",
       " 'cheering',\n",
       " 'godel',\n",
       " 'policenot',\n",
       " 'feast',\n",
       " 'usernick',\n",
       " 'recordfact',\n",
       " 'importantas',\n",
       " 'viktor',\n",
       " 'oaxaca',\n",
       " 'faithlessfaggotboy',\n",
       " 'wpvandal',\n",
       " 'despiteful',\n",
       " 'mistru',\n",
       " 'amoutn',\n",
       " 'ruddiman',\n",
       " 'alexfusco',\n",
       " 'wotan',\n",
       " 'kafkaesque',\n",
       " 'refastened',\n",
       " 'answeringislamorg',\n",
       " 'everyoneas',\n",
       " 'jobsworthlike',\n",
       " 'gotthardbahn',\n",
       " 'ghazarian',\n",
       " 'yngwie',\n",
       " 'templatetalkrtosection',\n",
       " 'salwey',\n",
       " 'conquer',\n",
       " 'peoplewelsh',\n",
       " 'domokuns',\n",
       " 'tidings',\n",
       " 'das',\n",
       " 'krakow',\n",
       " 'bupa',\n",
       " 'talksahanxwhen',\n",
       " 'beno',\n",
       " 'persuant',\n",
       " 'talkleopold',\n",
       " 'catching',\n",
       " 'scrapie',\n",
       " 'aproberts',\n",
       " 'samuell',\n",
       " 'boboltz',\n",
       " 'imagerivertrentnottinghamjpg',\n",
       " 'permanentlymuch',\n",
       " 'findadeathcom',\n",
       " 'pioneercourthouse',\n",
       " 'chilia',\n",
       " 'wppovneutral',\n",
       " 'shorlty',\n",
       " 'wellworked',\n",
       " 'nederzettingen',\n",
       " 'skal',\n",
       " 'informationstepen',\n",
       " 'ramjanmabhoomi',\n",
       " 'sendorfer',\n",
       " 'kawi',\n",
       " 'satsa',\n",
       " 'defeding',\n",
       " 'alwayz',\n",
       " 'restrictive',\n",
       " 'dionwr',\n",
       " 'complainees',\n",
       " 'unnumber',\n",
       " 'comicios',\n",
       " 'inbulgarian',\n",
       " 'dcbased',\n",
       " 'fantailed',\n",
       " 'talksmsarmad',\n",
       " 'biographyyouve',\n",
       " 'neofeudal',\n",
       " 'numbersif',\n",
       " 'discounting',\n",
       " 'technetium',\n",
       " 'dalwhinnie',\n",
       " 'hammett',\n",
       " 'wikipediawordstoavoid',\n",
       " 'mdash',\n",
       " 'notyetfiled',\n",
       " 'ishmaelies',\n",
       " 'aleichem',\n",
       " 'somhow',\n",
       " 'categorymedalists',\n",
       " 'ejnar',\n",
       " 'talkillyrian',\n",
       " 'masterfully',\n",
       " 'talkbar',\n",
       " 'demotingabusingdemeaningdejecting',\n",
       " 'cvilised',\n",
       " 'doomed',\n",
       " 'crowdsurfers',\n",
       " 'vitalist',\n",
       " 'judje',\n",
       " 'rationalwiki',\n",
       " 'nhbai',\n",
       " 'actualp',\n",
       " 'dbrodbeck',\n",
       " 'duuur',\n",
       " 'rather',\n",
       " 'pseudojustification',\n",
       " 'snorting',\n",
       " 'mcafees',\n",
       " 'pubchemid',\n",
       " 'fishserving',\n",
       " 'orissa',\n",
       " 'nergal',\n",
       " 'alves',\n",
       " 'gaons',\n",
       " 'offen',\n",
       " 'bayi',\n",
       " 'cunnig',\n",
       " 'pzrmd',\n",
       " 'purposei',\n",
       " 'dissociate',\n",
       " 'rubins',\n",
       " 'languagesuseful',\n",
       " 'btu',\n",
       " 'roerig',\n",
       " 'dietmar',\n",
       " 'enforces',\n",
       " 'wikipediaarticlesfordeletionmajorgarrett',\n",
       " 'templatetext',\n",
       " 'imagefinished',\n",
       " 'dmaze',\n",
       " 'lobstered',\n",
       " 'ramsay',\n",
       " 'fhs',\n",
       " 'daympscomb',\n",
       " 'sionics',\n",
       " 'footbal',\n",
       " 'computercontrolled',\n",
       " 'louie',\n",
       " 'suzanne',\n",
       " 'vanalising',\n",
       " 'kenyattas',\n",
       " 'wppilot',\n",
       " 'ada',\n",
       " 'scienceinformatics',\n",
       " 'hammersfan',\n",
       " 'layerthat',\n",
       " 'coachingrequests',\n",
       " 'appretiate',\n",
       " 'idrissa',\n",
       " 'eploration',\n",
       " 'honourlike',\n",
       " 'wpffd',\n",
       " 'blurryness',\n",
       " 'ckshyam',\n",
       " 'sprevjane',\n",
       " 'manioulating',\n",
       " 'warworkshop',\n",
       " 'workingclass',\n",
       " 'filetyrice',\n",
       " 'frigging',\n",
       " 'apodigm',\n",
       " 'mertz',\n",
       " 'devided',\n",
       " 'vremya',\n",
       " 'bwain',\n",
       " 'thermodynamically',\n",
       " 'encase',\n",
       " 'compacta',\n",
       " 'cyopu',\n",
       " 'airfoil',\n",
       " 'nucleotidetabatabtab',\n",
       " 'worley',\n",
       " 'ctholique',\n",
       " 'fetusgif',\n",
       " 'bahaullahs',\n",
       " 'sightedness',\n",
       " 'cburnett',\n",
       " 'subdueth',\n",
       " 'aeronautics',\n",
       " 'onenessthe',\n",
       " 'uckg',\n",
       " 'spazs',\n",
       " 'chenoweth',\n",
       " 'repeatedly',\n",
       " 'wisner',\n",
       " 'cogm',\n",
       " 'gareth',\n",
       " 'xav',\n",
       " 'fontam',\n",
       " 'dustyplasmas',\n",
       " 'instructive',\n",
       " 'merriweather',\n",
       " 'lariam',\n",
       " 'hads',\n",
       " 'tdii',\n",
       " 'baul',\n",
       " 'correctconnect',\n",
       " 'anarchofeminism',\n",
       " 'lessthanliterate',\n",
       " 'hystory',\n",
       " 'mty',\n",
       " 'stralsunds',\n",
       " 'infox',\n",
       " 'offensivelyyou',\n",
       " 'artilery',\n",
       " 'porra',\n",
       " 'coppied',\n",
       " 'avenuebroadway',\n",
       " 'lawyerssources',\n",
       " 'downed',\n",
       " 'overcome',\n",
       " 'mesiah',\n",
       " 'busines',\n",
       " 'motholo',\n",
       " 'newpapers',\n",
       " 'contemporarily',\n",
       " 'mussorgsky',\n",
       " 'rushmoore',\n",
       " 'vjdchauhan',\n",
       " 'hanly',\n",
       " 'policyconsensus',\n",
       " 'usertalksarsaparillareviewofdeletionofpageuserpresterjohn',\n",
       " 'inaugaration',\n",
       " 'oomph',\n",
       " 'directionawestdirectionbeast',\n",
       " 'tecnologie',\n",
       " 'leaveing',\n",
       " 'crossdressed',\n",
       " 'retires',\n",
       " 'aronoff',\n",
       " 'kolenkow',\n",
       " 'talkspontaneous',\n",
       " 'reilla',\n",
       " 'opinionsalthough',\n",
       " 'inflamatory',\n",
       " 'titiling',\n",
       " 'subterfugal',\n",
       " 'honkin',\n",
       " 'mistaking',\n",
       " 'jonc',\n",
       " 'undesire',\n",
       " 'talkxenobot',\n",
       " 'wpiinfo',\n",
       " 'dilly',\n",
       " 'beingthe',\n",
       " 'jian',\n",
       " 'dozer',\n",
       " 'obstruction',\n",
       " 'thegamealihotmailcom',\n",
       " 'sererniominkaa',\n",
       " 'euphrates',\n",
       " 'hervors',\n",
       " 'brandenburgwho',\n",
       " 'mki',\n",
       " 'photoplay',\n",
       " 'einb',\n",
       " 'resequenced',\n",
       " 'iescalpelretractor',\n",
       " 'scrotal',\n",
       " 'stenographer',\n",
       " 'friendsi',\n",
       " 'davidyork',\n",
       " 'favordefeating',\n",
       " 'adoring',\n",
       " 'tomodify',\n",
       " 'sidebar',\n",
       " 'fad',\n",
       " 'cultured',\n",
       " 'rotgin',\n",
       " 'bridgwater',\n",
       " 'sinas',\n",
       " 'bookofsecrets',\n",
       " 'hiw',\n",
       " 'monist',\n",
       " 'talkutah',\n",
       " 'sourcetabadvocate',\n",
       " 'reccords',\n",
       " 'stanbul',\n",
       " 'blinked',\n",
       " 'autohidden',\n",
       " 'regenhard',\n",
       " 'quayles',\n",
       " 'vandalismheader',\n",
       " 'eastland',\n",
       " 'angeloi',\n",
       " 'martling',\n",
       " 'melisandre',\n",
       " 'nlparelhoenders',\n",
       " 'character',\n",
       " 'znypes',\n",
       " 'cydartmouth',\n",
       " 'simensis',\n",
       " 'bitdefender',\n",
       " 'neighboursthat',\n",
       " 'carabinieri',\n",
       " 'autoinstalling',\n",
       " 'deletiondatedpageonepuretimestamp',\n",
       " 'pecautions',\n",
       " 'hhvm',\n",
       " 'ghostrider',\n",
       " 'hdtp',\n",
       " 'andrewparodiaolcom',\n",
       " 'phonomenon',\n",
       " 'baichung',\n",
       " 'mattering',\n",
       " 'assonine',\n",
       " 'slightestall',\n",
       " 'bartmann',\n",
       " 'oltean',\n",
       " 'provokes',\n",
       " 'mqs',\n",
       " 'contributionsfeed',\n",
       " 'imagecaptured',\n",
       " 'backwhen',\n",
       " 'girly',\n",
       " 'perfume',\n",
       " 'domanfleet',\n",
       " 'kigoy',\n",
       " 'tannin',\n",
       " 'komodo',\n",
       " 'funketeers',\n",
       " 'wikiprojectvideogames',\n",
       " 'cruelty',\n",
       " 'austerlitz',\n",
       " 'brecker',\n",
       " 'mabe',\n",
       " 'philomela',\n",
       " 'isempty',\n",
       " 'unnessacary',\n",
       " 'noma',\n",
       " 'nicodemus',\n",
       " 'rachavaruandhra',\n",
       " 'encyklopedia',\n",
       " 'banyan',\n",
       " 'murmur',\n",
       " 'pussbroad',\n",
       " 'stierch',\n",
       " 'wpae',\n",
       " 'forgetful',\n",
       " 'prayerfortheworld',\n",
       " 'pagodaonlinethe',\n",
       " 'keegscee',\n",
       " 'ungulatesartiodactyla',\n",
       " 'selfexciting',\n",
       " 'pagestructures',\n",
       " 'commonscategorystar',\n",
       " 'epitomizes',\n",
       " 'hdayejr',\n",
       " 'kuk',\n",
       " 'decadal',\n",
       " 'multiprotocol',\n",
       " 'categoryprocannabis',\n",
       " 'niraikanai',\n",
       " 'killedar',\n",
       " 'nigguuuh',\n",
       " 'birnbach',\n",
       " 'indifinite',\n",
       " 'bondscheater',\n",
       " 'archaelogicale',\n",
       " 'messagebut',\n",
       " 'dissimilar',\n",
       " 'mindovermatter',\n",
       " 'zhytomyr',\n",
       " 'makewe',\n",
       " 'usertalkdeltaquad',\n",
       " 'copyrightfree',\n",
       " 'textpasting',\n",
       " 'enriches',\n",
       " 'nichiren',\n",
       " 'withoversee',\n",
       " 'publicshing',\n",
       " 'francophonieandrophilie',\n",
       " 'beypnd',\n",
       " 'castesthen',\n",
       " 'morty',\n",
       " 'ball',\n",
       " 'bokenkotter',\n",
       " 'nonferrous',\n",
       " 'oppened',\n",
       " 'faceupjimbosassbuttkissing',\n",
       " 'schwabs',\n",
       " 'trancenet',\n",
       " 'trollop',\n",
       " 'rntalkcomubbthreadsshowflatphpboarddvdtalknumber',\n",
       " 'whatsoeverthey',\n",
       " 'imageworship',\n",
       " 'bestuzhevaryumina',\n",
       " 'imagesmosaic',\n",
       " 'happned',\n",
       " 'pragapoludnie',\n",
       " 'burhanuddins',\n",
       " 'deluding',\n",
       " 'confuses',\n",
       " 'dullard',\n",
       " 'terrestrially',\n",
       " 'thereon',\n",
       " 'regrouped',\n",
       " 'kemo',\n",
       " 'medition',\n",
       " 'osophical',\n",
       " 'townsend',\n",
       " 'vacuuminsulatedevaporator',\n",
       " 'edjohnson',\n",
       " 'parjay',\n",
       " 'goguryeo',\n",
       " 'editsreverts',\n",
       " 'joergenb',\n",
       " 'oakwood',\n",
       " 'messiahking',\n",
       " 'numorous',\n",
       " 'highrez',\n",
       " 'conspires',\n",
       " 'serbin',\n",
       " 'angiospermschhe',\n",
       " 'adelaide',\n",
       " 'ealiest',\n",
       " 'filepaintbrush',\n",
       " 'legalun',\n",
       " 'cabinetlevel',\n",
       " 'splitsville',\n",
       " 'photographas',\n",
       " 'eliminationoctober',\n",
       " 'talkfigureskatingfan',\n",
       " 'tangutology',\n",
       " 'sates',\n",
       " 'talktasc',\n",
       " 'quiche',\n",
       " 'bikers',\n",
       " 'afon',\n",
       " 'derision',\n",
       " 'tretiakov',\n",
       " 'vinditice',\n",
       " 'helmsburton',\n",
       " 'brandcloseoutcom',\n",
       " 'foundationresolutionlicensing',\n",
       " 'bruddah',\n",
       " 'embody',\n",
       " 'dhanunjaya',\n",
       " 'thatm',\n",
       " 'partisanism',\n",
       " 'niggerfags',\n",
       " 'nerdloser',\n",
       " 'piestany',\n",
       " 'porumbacu',\n",
       " 'misapprehending',\n",
       " 'dep',\n",
       " 'recruit',\n",
       " 'factitious',\n",
       " 'lynda',\n",
       " 'possesions',\n",
       " 'thiaw',\n",
       " 'obse',\n",
       " 'cabalhood',\n",
       " 'demonination',\n",
       " 'geriatric',\n",
       " 'animpaamo',\n",
       " 'boortz',\n",
       " 'declineconsidering',\n",
       " 'iloveyourteecom',\n",
       " 'parti',\n",
       " 'tabulating',\n",
       " 'quarterinch',\n",
       " 'sitewhy',\n",
       " 'energyportal',\n",
       " 'finderskeepers',\n",
       " 'blackguard',\n",
       " 'frisbie',\n",
       " 'imagebrookelittle',\n",
       " 'adapter',\n",
       " 'peaple',\n",
       " 'irishnationalist',\n",
       " 'butte',\n",
       " 'strikeres',\n",
       " 'moorgate',\n",
       " 'userrumplestiltskin',\n",
       " 'karlsson',\n",
       " 'fron',\n",
       " 'werwolf',\n",
       " 'reticence',\n",
       " 'dobie',\n",
       " 'distributing',\n",
       " 'promissory',\n",
       " 'deloitte',\n",
       " 'timenoi',\n",
       " 'redisruption',\n",
       " 'tabbanamalipur',\n",
       " 'fuchsscratch',\n",
       " 'resurgence',\n",
       " 'jewscollectively',\n",
       " 'scip',\n",
       " 'clarkson',\n",
       " 'darthblinky',\n",
       " 'dgt',\n",
       " 'connote',\n",
       " 'semiromance',\n",
       " 'mulberry',\n",
       " 'docheuh',\n",
       " 'wellfounded',\n",
       " 'emilio',\n",
       " 'dennv',\n",
       " 'shwikipedia',\n",
       " 'jamescantor',\n",
       " 'basicscience',\n",
       " 'strathclyde',\n",
       " 'overgrazing',\n",
       " 'fretting',\n",
       " 'geriatrick',\n",
       " 'inaccuracy',\n",
       " 'stansfield',\n",
       " 'requestedpoliticians',\n",
       " 'easonite',\n",
       " 'wobble',\n",
       " 'seyfrieds',\n",
       " 'bin',\n",
       " 'irresistibly',\n",
       " 'highcirculation',\n",
       " 'talkbaiji',\n",
       " 'israelhezbollah',\n",
       " 'dullness',\n",
       " 'matsukis',\n",
       " 'madhuri',\n",
       " 'stef',\n",
       " 'xmitter',\n",
       " 'reemerged',\n",
       " 'overuse',\n",
       " 'controlfreaks',\n",
       " 'spitball',\n",
       " 'castle',\n",
       " 'ocbvious',\n",
       " 'intimating',\n",
       " 'incarcerated',\n",
       " 'colum',\n",
       " 'gaarticles',\n",
       " 'hardon',\n",
       " 'diritti',\n",
       " 'likening',\n",
       " 'melatonin',\n",
       " 'winglets',\n",
       " 'photographjpg',\n",
       " 'markovitz',\n",
       " 'cayambes',\n",
       " 'benburch',\n",
       " 'yugo',\n",
       " 'clubz',\n",
       " 'shuart',\n",
       " 'tsongas',\n",
       " 'lyke',\n",
       " 'lfie',\n",
       " 'nowgot',\n",
       " 'weres',\n",
       " 'absencebut',\n",
       " 'vgchartz',\n",
       " 'preist',\n",
       " 'fixe',\n",
       " 'practicly',\n",
       " 'wpcitations',\n",
       " 'countykentuckya',\n",
       " 'filenameskreamruttensampleogg',\n",
       " 'firebending',\n",
       " 'sikhiwiki',\n",
       " 'unuser',\n",
       " 'useroldwindybears',\n",
       " 'ingratiating',\n",
       " 'merges',\n",
       " 'della',\n",
       " 'editingpov',\n",
       " 'disambaugation',\n",
       " 'afican',\n",
       " 'distros',\n",
       " 'nrk',\n",
       " 'sportsfans',\n",
       " 'almeriajpg',\n",
       " 'mitta',\n",
       " 'asion',\n",
       " 'rtinp',\n",
       " 'yoyoyoyoyo',\n",
       " 'funkyflytalk',\n",
       " 'admiralmarshal',\n",
       " 'wikipediasandboxsandbox',\n",
       " 'jasveer',\n",
       " 'peghsip',\n",
       " 'eggheaded',\n",
       " 'typoooooooo',\n",
       " 'patroller',\n",
       " 'du',\n",
       " 'mitotarget',\n",
       " 'deletinh',\n",
       " 'haris',\n",
       " 'acording',\n",
       " 'sophiakorichi',\n",
       " 'vale',\n",
       " 'richfield',\n",
       " 'webdesign',\n",
       " 'userfipplet',\n",
       " 'userdemiurge',\n",
       " 'articoli',\n",
       " 'pinpoint',\n",
       " 'scrutinizing',\n",
       " 'rust',\n",
       " 'treerelated',\n",
       " 'spaders',\n",
       " 'swa',\n",
       " 'ignored',\n",
       " 'virtualizing',\n",
       " 'neophyte',\n",
       " 'yassky',\n",
       " 'talkasteriskstarsplat',\n",
       " 'rohrbachers',\n",
       " 'guesswork',\n",
       " 'brittanica',\n",
       " 'flavius',\n",
       " 'universitysanctioned',\n",
       " 'khammamtab',\n",
       " 'gist',\n",
       " 'tku',\n",
       " 'elsker',\n",
       " 'carrs',\n",
       " 'wpaum',\n",
       " 'inteferring',\n",
       " 'grandm',\n",
       " 'yume',\n",
       " 'aokay',\n",
       " 'ivor',\n",
       " 'chipr',\n",
       " 'diggus',\n",
       " 'stickamcom',\n",
       " 'propose',\n",
       " 'delimiter',\n",
       " 'kril',\n",
       " 'alliancethe',\n",
       " 'seaeast',\n",
       " 'ibadi',\n",
       " 'thomazi',\n",
       " 'bestowed',\n",
       " 'wpfauna',\n",
       " 'sudoghost',\n",
       " 'boomtown',\n",
       " 'punchier',\n",
       " 'truely',\n",
       " 'fictionalcities',\n",
       " 'selten',\n",
       " 'galbatorix',\n",
       " 'kearsney',\n",
       " 'boxsince',\n",
       " 'jarvis',\n",
       " 'jweiss',\n",
       " 'categoryquincy',\n",
       " 'nstedlowery',\n",
       " 'magnaninimous',\n",
       " 'samira',\n",
       " 'convergence',\n",
       " 'boundary',\n",
       " 'scrutons',\n",
       " 'demockracy',\n",
       " 'nonagreed',\n",
       " 'adays',\n",
       " 'scibaby',\n",
       " 'dyatlov',\n",
       " 'panchjanya',\n",
       " 'cag',\n",
       " 'krai',\n",
       " 'subsubcategories',\n",
       " 'jouranistic',\n",
       " 'decorum',\n",
       " ...}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See the number of unique words in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "203143"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_to_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two dictionaries is successfully made gives each unique word in data an unique number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function is to let the length of each row is the same to enter the transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function takes column name and data , max sequence lenght , dictionary token_to_index , padding is set = 0 as default\n",
    "def tokens_to_indices_with_padding(tokens, max_seq_length, token_to_index, padding_value=0):\n",
    "    # Convert tokens to indices\n",
    "    # moves over each word in the text and check if it in the dictionary \n",
    "    # if yes , takes the corresponding index and put the word in the list\n",
    "    indices = [token_to_index[token] for token in tokens if token in token_to_index]\n",
    "    \n",
    "    # Apply padding or truncation\n",
    "    # subtract the lenght of indices list from the max sequence lenght\n",
    "    # to determine the number of padding elements\n",
    "    pad_length = max_seq_length - len(indices)\n",
    "\n",
    "    # create a list and take the first \"max_seq_length\" elements from indices\n",
    "    # and add padding * pad length   \n",
    "    padded_indices = indices[:max_seq_length] + [padding_value] * pad_length\n",
    "\n",
    "    return padded_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to count number of words in lemmatized data to put a number for variable max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the maximum word counts across all rows: 33.59922477669509\n"
     ]
    }
   ],
   "source": [
    "# function to count words\n",
    "def count_words(text):\n",
    "    # split a sentence to words\n",
    "    words = text.split()\n",
    "    # return the length of words\n",
    "    return len(words)\n",
    "\n",
    "# apply function on lemmatization column and make new column\n",
    "df['max_word_count'] = df['lemmatization_column'].apply(count_words)\n",
    "\n",
    "# calculate the mean of the new column\n",
    "mean_max_word_count = df['max_word_count'].mean()\n",
    "\n",
    "print(f\"Mean of the maximum word counts across all rows: {mean_max_word_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I can put to max_seq_len = 50 because it is near the mean of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply function on data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the maximum sequence length\n",
    "max_seq_length = 50  \n",
    "# This lets each row has a the maximum sequence length of 50\n",
    "\n",
    "\n",
    "# make a function to make padding around each row\n",
    "def apply_padding(row):\n",
    "    # Split lemmatization column into words\n",
    "    lemmatized_tokens = row['lemmatization_column'].split()  \n",
    "    # applies tokens_to_indices_with_padding function on each row\n",
    "    padded_indices = tokens_to_indices_with_padding(lemmatized_tokens, max_seq_length, token_to_index)\n",
    "    return padded_indices\n",
    "\n",
    "# apply the function to make padding column\n",
    "df['padding_column'] = df.apply(apply_padding, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>tokenized_column</th>\n",
       "      <th>lemmatization_column</th>\n",
       "      <th>offensive</th>\n",
       "      <th>max_word_count</th>\n",
       "      <th>padding_column</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[explanation, edits, made, username, hardcore,...</td>\n",
       "      <td>explanation edits made username hardcore metal...</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>daww matches background colour seemingly stuck...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[daww, matches, background, colour, seemingly,...</td>\n",
       "      <td>daww match background colour seemingly stuck t...</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>[24, 25, 26, 27, 28, 29, 30, 20, 31, 32, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[hey, man, really, trying, edit, war, guy, con...</td>\n",
       "      <td>hey man really trying edit war guy constantly ...</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>[33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>make real suggestions improvement wondered sec...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[make, real, suggestions, improvement, wondere...</td>\n",
       "      <td>make real suggestion improvement wondered sect...</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>[51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[sir, hero, chance, remember, page]</td>\n",
       "      <td>sir hero chance remember page</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>[92, 93, 94, 95, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157627</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>second time asking view completely contradicts...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[second, time, asking, view, completely, contr...</td>\n",
       "      <td>second time asking view completely contradicts...</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>[1204, 605, 2776, 469, 1103, 10162, 6302, 1153...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157628</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>ashamed horrible thing put talk page</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[ashamed, horrible, thing, put, talk, page]</td>\n",
       "      <td>ashamed horrible thing put talk page</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>[8170, 2705, 586, 990, 20, 21, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157629</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>spitzer umm actual article prostitution ring c...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[spitzer, umm, actual, article, prostitution, ...</td>\n",
       "      <td>spitzer umm actual article prostitution ring c...</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>[88698, 10565, 49, 82, 7281, 1892, 24412, 3595...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157630</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>looks like actually put speedy first version d...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[looks, like, actually, put, speedy, first, ve...</td>\n",
       "      <td>look like actually put speedy first version de...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>[276, 277, 760, 990, 182, 74, 487, 178, 276, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157631</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>really think understand came idea bad right aw...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[really, think, understand, came, idea, bad, r...</td>\n",
       "      <td>really think understand came idea bad right aw...</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>[35, 62, 450, 333, 2322, 410, 986, 1677, 1681,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>157632 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "0       0000997932d777bf  explanation edits made username hardcore metal...   \n",
       "1       000103f0d9cfb60f  daww matches background colour seemingly stuck...   \n",
       "2       000113f07ec002fd  hey man really trying edit war guy constantly ...   \n",
       "3       0001b41b1c6bb37e  make real suggestions improvement wondered sec...   \n",
       "4       0001d958c54c6e35                      sir hero chance remember page   \n",
       "...                  ...                                                ...   \n",
       "157627  ffe987279560d7ff  second time asking view completely contradicts...   \n",
       "157628  ffea4adeee384e90               ashamed horrible thing put talk page   \n",
       "157629  ffee36eab5c267c9  spitzer umm actual article prostitution ring c...   \n",
       "157630  fff125370e4aaaf3  looks like actually put speedy first version d...   \n",
       "157631  fff46fc426af1f9a  really think understand came idea bad right aw...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0           0             0        0       0       0              0   \n",
       "1           0             0        0       0       0              0   \n",
       "2           0             0        0       0       0              0   \n",
       "3           0             0        0       0       0              0   \n",
       "4           0             0        0       0       0              0   \n",
       "...       ...           ...      ...     ...     ...            ...   \n",
       "157627      0             0        0       0       0              0   \n",
       "157628      0             0        0       0       0              0   \n",
       "157629      0             0        0       0       0              0   \n",
       "157630      0             0        0       0       0              0   \n",
       "157631      0             0        0       0       0              0   \n",
       "\n",
       "                                         tokenized_column  \\\n",
       "0       [explanation, edits, made, username, hardcore,...   \n",
       "1       [daww, matches, background, colour, seemingly,...   \n",
       "2       [hey, man, really, trying, edit, war, guy, con...   \n",
       "3       [make, real, suggestions, improvement, wondere...   \n",
       "4                     [sir, hero, chance, remember, page]   \n",
       "...                                                   ...   \n",
       "157627  [second, time, asking, view, completely, contr...   \n",
       "157628        [ashamed, horrible, thing, put, talk, page]   \n",
       "157629  [spitzer, umm, actual, article, prostitution, ...   \n",
       "157630  [looks, like, actually, put, speedy, first, ve...   \n",
       "157631  [really, think, understand, came, idea, bad, r...   \n",
       "\n",
       "                                     lemmatization_column  offensive  \\\n",
       "0       explanation edits made username hardcore metal...          0   \n",
       "1       daww match background colour seemingly stuck t...          0   \n",
       "2       hey man really trying edit war guy constantly ...          0   \n",
       "3       make real suggestion improvement wondered sect...          0   \n",
       "4                           sir hero chance remember page          0   \n",
       "...                                                   ...        ...   \n",
       "157627  second time asking view completely contradicts...          0   \n",
       "157628               ashamed horrible thing put talk page          0   \n",
       "157629  spitzer umm actual article prostitution ring c...          0   \n",
       "157630  look like actually put speedy first version de...          0   \n",
       "157631  really think understand came idea bad right aw...          0   \n",
       "\n",
       "        max_word_count                                     padding_column  \n",
       "0                   23  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...  \n",
       "1                   10  [24, 25, 26, 27, 28, 29, 30, 20, 31, 32, 0, 0,...  \n",
       "2                   21  [33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 4...  \n",
       "3                   48  [51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 6...  \n",
       "4                    5  [92, 93, 94, 95, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "...                ...                                                ...  \n",
       "157627              23  [1204, 605, 2776, 469, 1103, 10162, 6302, 1153...  \n",
       "157628               6  [8170, 2705, 586, 990, 20, 21, 0, 0, 0, 0, 0, ...  \n",
       "157629               8  [88698, 10565, 49, 82, 7281, 1892, 24412, 3595...  \n",
       "157630               9  [276, 277, 760, 990, 182, 74, 487, 178, 276, 0...  \n",
       "157631              18  [35, 62, 450, 333, 2322, 410, 986, 1677, 1681,...  \n",
       "\n",
       "[157632 rows x 13 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A new column is made called padded_column making the text data numbers not text to enter the Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Transfomer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The transformer is started from a paper called \"attention is all you need\" .\n",
    "The transformer containes many classes like :\n",
    "\n",
    "1.  Positional Encoding: Positional encoding adds information about the position of each token in the sequence to the embeddings, helping the Transformer model understand the sequential order of the input data.\n",
    "\n",
    "2. Position-wise Feed-Forward Networks: After attention, each position's representation undergoes a position-wise feed-forward network, which applies a set of learnable transformations to capture non-linear patterns and relationships within the encoded information.\n",
    "\n",
    "3.  Multi-Head Attention: This step in the Transformer model involves computing multiple attention heads in parallel, enabling the model to focus on different parts of the input sequence simultaneously, enhancing its ability to capture complex relationships.\n",
    "\n",
    "4. Encoder: The encoder consists of multiple layers of Multi-Head Attention and Position-wise Feed-Forward Networks. Each layer incrementally refines the input representations, allowing the model to capture both local and global dependencies in the data.\n",
    "\n",
    "5. Decoder: The decoder in a Transformer model is composed of several layers, each with its own Multi-Head Attention mechanism and Position-wise Feed-Forward Networks. These layers generate the output sequence step by step, attending to the encoder's output and previously generated tokens, facilitating sequence-to-sequence tasks like machine translation or text generation.\n",
    "\n",
    "6. Transformer: The Transformer class is the combination of all classes with each other "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    # Constructor for the Positional Encoding class \n",
    "    # that calls max sequence length and dimension of model\n",
    "    def __init__(self, d_model, max_seq_len):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # Initialize a matrix to hold positional encodings\n",
    "        # torch matrix of zeros  \n",
    "        self.pe = torch.zeros(max_seq_len, d_model)\n",
    "        \n",
    "        # Create a tensor representing the positions within the sequence\n",
    "        # make 2 dimensional array from zero to max_seq_len-1 \n",
    "        position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # makes the denumerator of the formula\n",
    "        # make a tensor from to dmodel that moves by 2\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * (-np.log(10000.0) / d_model))\n",
    "        \n",
    "        # Calculate the sine and cosine values for each position and dimension\n",
    "        # sin for even positions and cosine for odd indices\n",
    "        self.pe[:, 0::2] = torch.sin(position * div_term) # even\n",
    "        self.pe[:, 1::2] = torch.cos(position * div_term) # odd\n",
    "        \n",
    "        # Add an extra dimension to the positional encodings to move around all batch of sentences\n",
    "        self.pe = self.pe.unsqueeze(0)\n",
    "\n",
    "    # Forward method for the Positional Encoding class\n",
    "    def forward(self, x):\n",
    "        # Add positional encodings to the input tensor\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeedForward class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    # Constructor for the FeedForward class\n",
    "    # that calls dimension of model and feedforward dimension\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        # make a fully connected layer input nodes = dimension of model \n",
    "        # output nodes = feedforward dimension\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        \n",
    "        # make a fully connected layer input nodes = feedforward dimension\n",
    "        # output nodes = dimension of model\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    # Forward method for the FeedForward class\n",
    "    def forward(self, x):\n",
    "        # add Relu as activation function for fc1\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        # no activation function on fc2\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    # Constructor for the Multi-Head Attention class \n",
    "    # that calls dimension of model and number of heads\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        # Initialize dimension of model \n",
    "        self.d_model = d_model \n",
    "\n",
    "        # Initialize Number of  heads\n",
    "        self.n_heads = n_heads \n",
    "\n",
    "        # Initialize the dimension of each attension head\n",
    "        self.head_dim = d_model // n_heads  \n",
    "\n",
    "        # Linear transformations for Query, Key, and Value\n",
    "        # W_Q is for query that that will have a of shape (seq , d_model)\n",
    "        self.W_Q = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # W_K is for key that will have a of shape (seq , d_model)\n",
    "        self.W_K = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # W_V is for value that will have a of shape (seq , d_model)\n",
    "        self.W_V = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Output and will make the output shape (seq , d_model)\n",
    "        self.W_O = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Q, K, and V are the Query, Key, and Value tensors\n",
    "        \n",
    "        # Get the batch size from the input tensor of Query\n",
    "        batch_size = Q.shape[0]\n",
    "\n",
    "        # Project the input Query, Key, and Value tensors\n",
    "        # multiply Q by w_Q to get output shape (seq , d_model)\n",
    "        Q = self.W_Q(Q)\n",
    "        # multiply K by w_K to get output shape (seq , d_model)\n",
    "        K = self.W_K(K)\n",
    "        # multiply V by w_V to get output shape (seq , d_model)\n",
    "        V = self.W_V(V)\n",
    "\n",
    "        # Divide the Query Q matrix to smaller matrices to give each smaller matrix to a different matrix\n",
    "        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim)\n",
    "        # Divide the Key K matrix to smaller matrices to give each smaller matrix to a different matrix\n",
    "        K = K.view(batch_size, -1, self.n_heads, self.head_dim)\n",
    "        # Divide the value V matrix to smaller matrices to give each smaller matrix to a different matrix\n",
    "        V = V.view(batch_size, -1, self.n_heads, self.head_dim)\n",
    "\n",
    "        # make the matrix multiplication on Query and Key for each head\n",
    "        # bqhd represents the dimensions of tensor Query in the form of (batch size, sequence length, number of attention heads, dimension of key vectors)\n",
    "        # bkhd represents the dimensions of tensor Key in the form of (batch size, sequence length, number of attention heads, dimension of key vectors)\n",
    "        # bhqk represents the output dimensions in the form of (batch size, number of attention heads, query sequence length, key sequence length)\n",
    "        QK = torch.einsum('bqhd,bkhd->bhqk', Q, K)\n",
    "\n",
    "        # get the attention scores by dividing the QK by square root of self.head_dim\n",
    "        scores = QK / torch.sqrt(torch.tensor(self.head_dim).float())\n",
    "\n",
    "        # Apply the attention mask if not set to None \n",
    "        if mask is not None:\n",
    "            # make the mask match the dimension of the scores tensor\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
    "            # replace where the mask = zero by large negative number\n",
    "            scores.masked_fill_(mask == 0, -1e9)\n",
    "\n",
    "        # make softmax to get attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # Compute matrix multiplication of attention_weights and Values\n",
    "        # bhqv represents the dimensions of the attention_weights tensor in the form of (batch size, number of attention heads, query sequence length, value dimension)\n",
    "        # bvhd represents the dimensions of the V tensor in the form of (batch size, sequence length, number of attention heads, dimension of value vectors)\n",
    "        # bqhd represents the output dimensions in the form of (batch size, query sequence length, number of attention heads, dimension of value vectors)\n",
    "        out = torch.einsum('bhqv,bvhd->bqhd', attention_weights, V)\n",
    "\n",
    "        # reshapes the tensor to combine the num_heads and head_dimension\n",
    "        out = out.reshape(batch_size, -1, self.d_model)\n",
    "\n",
    "        # multiply W_O by out reshaped \n",
    "        # The result is a tensor of shape (batch_size, sequence_length)\n",
    "        out = self.W_O(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder Layer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    # Constructor for the Encoder Layer class \n",
    "    # that calls dimension of model and number of heads and feedforward dimension\n",
    "    def __init__(self, d_model, n_heads, d_ff):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        # call Multi-Head Attention class\n",
    "        self.self_attention = MultiHeadAttention(d_model, n_heads)\n",
    "        \n",
    "        # call Feed-Forward class\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        \n",
    "        # Initialize Layer Normalization for the dimension of model\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Initialize Layer Normalization for the dimension of model\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Multi-Head Self-Attention\n",
    "        attention_output = self.self_attention(x, x, x, mask)\n",
    "        \n",
    "        # Residual Connection for self-attention\n",
    "        x = x + attention_output\n",
    "        # Layer Normalization for self-attention\n",
    "        x = self.norm1(x)\n",
    "        \n",
    "        # Feed-Forward Neural Network\n",
    "        feed_forward_output = self.feed_forward(x)\n",
    "        \n",
    "        # Residual Connection for feed-forward\n",
    "        x = x + feed_forward_output\n",
    "        # Layer Normalization for feed-forward\n",
    "        x = self.norm2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    # Constructor for the Transfomer Encoder class \n",
    "    # that calls dimension of model and number of heads and feedforward dimension and number of layers\n",
    "    def __init__(self, num_layers, d_model, n_heads, d_ff):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        # Create a list of number encoder layers \n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model, n_heads, d_ff) for _ in range(num_layers)])\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        # for the number of layers \n",
    "        for layer in self.layers:\n",
    "            # make the input x which has shape of (batch_size, seq_len, d_model)\n",
    "            # mask to hide padded elements\n",
    "            x = layer(x, mask)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfomer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    # Constructor for the Transfomer class \n",
    "    # that calls dimension of model and number of heads and feedforward dimension and number of layers\n",
    "    # and size of vocabulary and max sequence length and number of output classes\n",
    "    def __init__(self, num_layers, d_model, n_heads, d_ff, input_vocab_size, max_seq_len, num_classes):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # make the embedding layer\n",
    "        self.embedding = nn.Embedding(input_vocab_size, d_model)\n",
    "\n",
    "        # call Positional Encoding class\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "\n",
    "        # call Transformer Encoder class\n",
    "        self.encoder = TransformerEncoder(num_layers, d_model, n_heads, d_ff)\n",
    "\n",
    "        # Initialize a fully conected layer \n",
    "        self.fc = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Pass input tokens into the embedding layer\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        # Apply positional encoding to the embedded output\n",
    "        x = self.positional_encoding(x)\n",
    "\n",
    "        # let the output of positional encoding pass through the encoder layer\n",
    "        x = self.encoder(x, mask)\n",
    "\n",
    "        # make average pooling on sequence length dimension on output of the encoder\n",
    "        x = x.mean(dim=1)  \n",
    "\n",
    "        # pass the output of average pooling through the fully connencted layer\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Didn't make an decoder layer because my goal to put a category to an input, not to generate a sequence-to-sequence as decoder layer does that is used for machine translation ; and also decoder makes unnecessary complexity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, valid_loader, criterion, optimizer, num_epochs, patience):\n",
    "    # make the model in training mode\n",
    "    model.train()\n",
    "\n",
    "    # Initialize the best validation loss to a large value\n",
    "    # that is for early stopping to not make overfitting\n",
    "    best_valid_loss = float('inf')  \n",
    "\n",
    "    # Initialize a counter for consecutive epochs with no improvement\n",
    "    # also for early stopping to not make overfitting \n",
    "    consecutive_no_improvement = 0  \n",
    "    \n",
    "    # for loop for the number of epoches\n",
    "    for epoch in range(num_epochs):\n",
    "        # Initialize the loss = zero\n",
    "        running_loss = 0.0\n",
    "        # Initialize the correct_predictions = zero\n",
    "        correct_predictions = 0\n",
    "        # Initialize the total_samples = zero\n",
    "        total_samples = 0\n",
    "        \n",
    "        # Loop in each batch in train data\n",
    "        for batch_idx, (inputs, targets) in enumerate(train_loader, 1):\n",
    "            # make zero to the grad \n",
    "            optimizer.zero_grad()  \n",
    "             # put mask to input not equal to zero\n",
    "            mask = (inputs != 0) \n",
    "            # Forward pass through the model\n",
    "            outputs = model(inputs, mask)  \n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, targets)  \n",
    "            # Backprop \n",
    "            loss.backward()  \n",
    "             # Update the weights\n",
    "            optimizer.step() \n",
    "            # add batch loss to epoch loss\n",
    "            running_loss += loss.item()  \n",
    "            \n",
    "            # store the prediction of model\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            # if predicted = target this correct prediction add it to the correct prediction\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "            # add the number of samples\n",
    "            total_samples += targets.size(0)\n",
    "            \n",
    "            # Print progress information the number of epoch done / the total number of epochs and number of batchs done / the total number of batchs\n",
    "            print(f\"\\rEpoch {epoch + 1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}\", end='', flush=True)\n",
    "\n",
    "        # Calculate the average loss for the epoch\n",
    "        epoch_loss = running_loss / len(train_loader)  \n",
    "        # Calculate the epoch accuracy\n",
    "        epoch_accuracy = 100 * correct_predictions / total_samples  \n",
    "        \n",
    "        \n",
    "       # make the model in evaluation mode for Validation\n",
    "        model.eval()  \n",
    "        # Initialize the loss = zero\n",
    "        valid_loss = 0.0\n",
    "        # Initialize the correct_predictions = zero\n",
    "        valid_correct = 0\n",
    "        # Initialize the total_samples = zero\n",
    "        valid_total = 0\n",
    "        \n",
    "        # donot make gradient \n",
    "        with torch.no_grad():\n",
    "            # Loop in each batch in validation data\n",
    "            for valid_inputs, valid_targets in valid_loader:\n",
    "                # put mask to input not equal to zero\n",
    "                valid_mask = (valid_inputs != 0)\n",
    "                # Forward pass through the model\n",
    "                valid_outputs = model(valid_inputs, valid_mask) \n",
    "                # Calculate validation loss\n",
    "                valid_loss_batch = criterion(valid_outputs, valid_targets)  \n",
    "                # add batch loss to epoch loss\n",
    "                valid_loss += valid_loss_batch.item()  \n",
    "                \n",
    "                # store the prediction of model\n",
    "                _, valid_predicted = torch.max(valid_outputs, 1)\n",
    "                # add the number of samples\n",
    "                valid_total += valid_targets.size(0)\n",
    "                # if predicted = target this correct prediction add it to the correct prediction\n",
    "                valid_correct += (valid_predicted == valid_targets).sum().item()\n",
    "\n",
    "        # Calculate the epoch accuracy\n",
    "        valid_accuracy = 100 * valid_correct / valid_total \n",
    "        # Calculate the average loss for the epoch\n",
    "        avg_valid_loss = valid_loss / len(valid_loader)  \n",
    "        \n",
    "        # print train loss and acc for validation and train data for each epoch\n",
    "        print(f\"\\rEpoch {epoch + 1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_accuracy:.2f}%, \" \\\n",
    "              f\"Valid Loss: {avg_valid_loss:.4f}, Valid Accuracy: {valid_accuracy:.2f}%\")\n",
    "        \n",
    "        # for Early stopping \n",
    "        # Check if the validation loss has improved\n",
    "        if avg_valid_loss < best_valid_loss:\n",
    "            best_valid_loss = avg_valid_loss\n",
    "            consecutive_no_improvement = 0\n",
    "        else:\n",
    "            # if loss increased add one to the no improvement variable\n",
    "            consecutive_no_improvement += 1\n",
    "        \n",
    "        # If there is no improvement variable exceeded the patient\n",
    "        if consecutive_no_improvement >= patience:\n",
    "            print(f\"Early stopping after {epoch + 1} epochs with no improvement.\")\n",
    "            # stop the function\n",
    "            break\n",
    "        \n",
    "        # make the model in training mode\n",
    "        model.train()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, criterion):\n",
    "    # make the model in evaluation mode\n",
    "    model.eval() \n",
    "    # Initialize the correct_predictions = zero\n",
    "    correct = 0  \n",
    "    # Initialize the total_samples = zero\n",
    "    total = 0\n",
    "    # Initialize the loss = zero\n",
    "    test_loss = 0.0  \n",
    "    # array for all predicted values\n",
    "    all_predicted = []\n",
    "    # array for all true values\n",
    "    all_targets = []\n",
    "\n",
    "    # donot make gradient \n",
    "    with torch.no_grad():\n",
    "        # Loop in each batch in test data\n",
    "        for inputs, targets in test_loader:\n",
    "            # put mask to input not equal to zero\n",
    "            mask = (inputs != 0) \n",
    "            # Forward pass through the model\n",
    "            outputs = model(inputs, mask) \n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, targets)\n",
    "            # add batch loss to loss\n",
    "            test_loss += loss.item()  \n",
    "            \n",
    "            # store the prediction of model\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            # add the number of samples\n",
    "            total += targets.size(0)\n",
    "            # if predicted = target this correct prediction add it to the correct prediction\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "            # add predicted values\n",
    "            all_predicted.extend(predicted.tolist())\n",
    "            # add true values\n",
    "            all_targets.extend(targets.tolist())\n",
    "    \n",
    "    # Calculate test accuracy\n",
    "    accuracy = 100 * correct / total  \n",
    "    # Calculate the average test loss\n",
    "    avg_test_loss = test_loss / len(test_loader)  \n",
    "    \n",
    "    # print test loss and acc \n",
    "    print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "    # print the classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    report = classification_report(all_targets, all_predicted)\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize train , validation , and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert padding column to numpy array \n",
    "padding_array = np.array(df['padding_column'].tolist())\n",
    "\n",
    "# convert padded array to tensor\n",
    "X = torch.from_numpy(padding_array).long()\n",
    "\n",
    "# convert offensive column that has classification to tensor\n",
    "y = torch.from_numpy(df['offensive'].values).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train, test \n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "# split test data into test and validation\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize batch size\n",
    "batch_size = 64\n",
    "\n",
    "# make tensor train data \n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "# make train data iterable and make batchs\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# make tensor validation data \n",
    "valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "# make validation data iterable and make batchs\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "\n",
    "# make tensor test data \n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "# make test data iterable and make batchs\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of layers\n",
    "num_layers = 6\n",
    "\n",
    "# dimension of the model\n",
    "d_model = 512\n",
    "\n",
    "# number of attension heads\n",
    "n_heads = 8\n",
    "\n",
    "# feedforward dimension \n",
    "d_ff = 2048 \n",
    "\n",
    "# vocab size = length of dictionary token_to_index or index_to_token they have same length\n",
    "input_vocab_size = len(token_to_index)  \n",
    "\n",
    "# make max sequence length as above\n",
    "max_seq_len = 50  \n",
    "\n",
    "# good learning rate not small nor big\n",
    "lr = 0.0001\n",
    "\n",
    "# for binary classification so 2 classes\n",
    "num_classes = 2  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I tried different hyperparameters but those get the best loss and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model , optimizer , criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(num_layers, d_model, n_heads, d_ff, input_vocab_size, max_seq_len, num_classes)\n",
    "# cross entropy loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the number of epoches , patience for early stopping \n",
    "### Apply of training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8, Train Loss: 0.1688, Train Accuracy: 94.64%, Valid Loss: 0.1358, Valid Accuracy: 95.35%\n",
      "Epoch 2/8, Train Loss: 0.1145, Train Accuracy: 95.92%, Valid Loss: 0.1241, Valid Accuracy: 95.77%\n",
      "Epoch 3/8, Train Loss: 0.0827, Train Accuracy: 96.96%, Valid Loss: 0.1420, Valid Accuracy: 95.60%\n",
      "Epoch 4/8, Train Loss: 0.0465, Train Accuracy: 98.36%, Valid Loss: 0.1852, Valid Accuracy: 95.23%\n",
      "Epoch 5/8, Train Loss: 0.0283, Train Accuracy: 98.98%, Valid Loss: 0.1808, Valid Accuracy: 95.43%\n",
      "Early stopping after 5 epochs with no improvement.\n"
     ]
    }
   ],
   "source": [
    "# define number of epoches\n",
    "num_epochs = 8\n",
    "# define number of patience , which is the parameter for early stopping  if validation loss increased by this number will stop training\n",
    "patience = 3\n",
    "\n",
    "# train the model\n",
    "train(model, train_loader, valid_loader, criterion, optimizer, num_epochs, patience)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### the training accuracy increased in each epoch , validation loss was best in second epoch then increased which will lead to overfitting so I put early stopping to not lead to overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply of test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1847, Test Accuracy: 95.43%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97     28310\n",
      "           1       0.84      0.69      0.75      3217\n",
      "\n",
      "    accuracy                           0.95     31527\n",
      "   macro avg       0.90      0.84      0.86     31527\n",
      "weighted avg       0.95      0.95      0.95     31527\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "test(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### explaination of the output\n",
    "- Precision : measure of how many of the predicted positive instances were actually positive\n",
    "- Recall : measure of how many of the actual positive instances were correctly predicted as positive\n",
    "- F1-Score : harmonic mean of precision and recall\n",
    "- Support : number of instances in the true dataset that belong to each class.\n",
    "- Accuracy : the accuracy of model which is 95 %\n",
    "- Macro Avg : These values are the unweighted average of precision, recall, and F1-score for all classes \n",
    "- Sample Avg : These values are similar to the micro average but consider each sample equally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The model performers well with 95% accuracy \n",
    "#### percision and recall on class 0 indicates that perdictes this class very well ; also it perdictes class 1 good but not as good as class 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
